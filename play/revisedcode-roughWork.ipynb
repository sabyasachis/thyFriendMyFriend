{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of NIPS 2017 paper, titled \"*Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation*\" ( [link](https://papers.nips.cc/paper/7057-thy-friend-is-my-friend-iterative-collaborative-filtering-for-sparse-matrix-estimation.pdf) and [arxiv](https://arxiv.org/pdf/1712.00710v1.pdf) ) for Global NIPS Paper implementation [Challenge](https://www.nurture.ai/nips-challenge). The authors have also created a short [video](https://www.youtube.com/watch?v=qxfDK44YuQE) for it.<br>\n",
    "For any questions related to this notebook, feel free to contact me at ssahoo.infinity@gmail.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this Notebook:\n",
    "Following are some important points/guidelines/assumptions to keep in mind while navigating through this notebook:\n",
    "- This notebook consists of sections numbered by Roman numerals\n",
    "- Brief description about all sections:\n",
    "  - I: Model preparation: Changes to be made to rating matrix(dataset) before we can apply the algorithms\n",
    "  - II: Algorithm Details: All the algorithms described in the paper are implemented here.\n",
    "  - III: Other important functions:\n",
    "    - Data Handling: for data manipulation\n",
    "    - Substitutes: methods which can be used in place of (algorithm) methods in paper\n",
    "    - Evaluation: for evaulation of recommender system\n",
    "  - IV: Test script/Experiments: testing using a dataset\n",
    "- If a function/variable has suffix as \"_csr\", it refers to CSR (Compressed Sparse Row) data. Else, if it has a suffix as \"_matrix\", it refers to 2D (numpy) matrix data.\n",
    "- The dataset ratings are assumed to be integers; to be modified in future\n",
    "- data_csr ensures that user_id and item_id start from 0 by taking in FIRST_INDEX as a global variable\n",
    "- All the datasets are symmetricized and normalized before we begin applying the algorithm\n",
    "- There are some dataset parameters like FIRST_INDEX, USERS, etc which will be automatically detected from dataset. Hence, they have been set as -1 for default condition. To overload automatic detection, provide a value (but we recommend you not to overload).\n",
    "- Brief description about hyperparameters:\n",
    "  - RADIUS : this controls the size of neighborhood. It is the distance from vertex u to vertices i (at neighborhood boundary). Setting large/small values for RADIUS might reduce the average number of neighbors per vertex, resulting suboptimal overlap of vertices. To set optimal RADIUS, you can use output from 'describe_neighbor_count' function to understand how varying RADIUS affects neighborhood size.\n",
    "  - THRESHOLD : this controls the final set of vertices which are considered for individual rating estimation. Setting this too high will make smaller the set of vertices being considered and vice versa. To set optimal threshold, you can use output from 'describe_distance_matrix' function to understand how THRESHOLD affects the size of this set.\n",
    "  - UNPRED_RATING : average rating for user-item pair, for which the algorithm could not make an estimate for the rating.\n",
    "  - TRAIN_TEST_SPLIT : %age of test dataset.\n",
    "  - C1 : %age of edges in train dataset going to $E1$ for expanding the neighborhood (step 2).\n",
    "  - C2 : %age of edges in train dataset going to $E2$ for distance computation between the vertices (step 3). Also, please note that 1 - C1 - C2 is %age of edges in train dataset going to $E3$ for rating estimation (step 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required modules required for complete notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(16, 44, 6, 708333)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Built and tested on python2\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "import sys\n",
    "from datetime import datetime\n",
    "datetime.now().time()     # (hour, min, sec, microsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: see that there are no duplicate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Model Notations\n",
    "Following are the symbols/notations used in the paper. The variables/notations used in this notebook have been discussed in Experiments section.\n",
    "\n",
    "$u$ = user <br>\n",
    "$i$ = item <br>\n",
    "$M$ = symmetric rating matrix of size $n \\times n$ (usually the dataset) <br>\n",
    "$E$ = set of $(u,i)$ where each user $u$ has rated an item $i$ also seen in the matrix $M$ (intuitively $E$ is edge set(matrix) between user and items. <br>\n",
    "$p$ = sparsity of $M$ i.e. (= #observed ratings in $M$ / total # ratings in $M$)<br>\n",
    "$r$ = radius, distance (in no of edges) between user $u$ and item $i$ at neighborhood boundary (look in step 2) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dataset Parameters'''\n",
    "################################################################################################################\n",
    "DATA_PATH = './ml-100k/u.data' # ml-100k data set has 100k ratings, 943 users and 1682 items\n",
    "DELIMITER = \"\\t\"               # tab separated or comma separated data format\n",
    "N_RATINGS = 100000\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These parameters will be detected automatically from dataset\n",
    "# -1 is for the default value\n",
    "FIRST_INDEX = -1\n",
    "USERS = -1\n",
    "ITEMS = -1\n",
    "SPARSITY = -1                  # 'p' in the equations\n",
    "UNOBSERVED = 0                 # default value in matrix for unobserved ratings; prefer to keep it 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To reduce size of csr for testing purpose\n",
    "# WARNING: ONLY TO BE USED FOR TESTING\n",
    "# (for real run, put SIZE_REDUCTION = False)\n",
    "SIZE_REDUCTION = True\n",
    "USER_LIMIT = 200\n",
    "ITEM_LIMIT = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 = 0.33 (default)\n",
      "c2 = 0.33 (default)\n",
      "c3 = 0.34 (default)\n",
      "Radius = 3\n",
      "Rating set for unpredicted ratings = 3\n",
      "TRAIN_TEST_SPLIT = 0.2 (default)\n"
     ]
    }
   ],
   "source": [
    "'''Hyperparameters'''\n",
    "# All the hyperparameters have default values\n",
    "#To use them, set the parameters as -1\n",
    "################################################################################################################\n",
    "TRAIN_TEST_SPLIT = -1                   # %age of test ratings wrt train rating ; value in between 0 and 1\n",
    "C1 = -1                                 # probability of edges in training set going to E1\n",
    "C2 = -1                                 # probability of edges in training set going to E2\n",
    "RADIUS = 3                              # radius of neighborhood, radius = # edges between start and end vertex\n",
    "UNPRED_RATING = 3                       # rating (normalized) for which we dont have predicted rating between 1 - 5\n",
    "THRESHOLD = 0.01                        # distance similarity threshold used for rating prediction\n",
    "################################################################################################################\n",
    "\n",
    "# checks on hyper parameters    \n",
    "if isinstance(C1, float) and isinstance(C2, float) and (C1 > 0) and (C2 > 0) and 1 - C1 - C2 > 0:\n",
    "    print('c1 = {}'.format(C1))\n",
    "    print('c2 = {}'.format(C2))\n",
    "    print('c3 = {}'.format(1-C1-C2))\n",
    "elif (C1 == -1) and (C2 == -1):\n",
    "    C1 = C2 = 0.33\n",
    "    print('c1 = {} (default)'.format(C1))\n",
    "    print('c2 = {} (default)'.format(C2))\n",
    "    print('c3 = {} (default)'.format(1-C1-C2))\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for C1 and C2')\n",
    "    \n",
    "if isinstance(RADIUS, int) and RADIUS > 0:\n",
    "    print('Radius = {}'.format(RADIUS))\n",
    "elif RADIUS == -1:\n",
    "    print('Radius = default value as per paper')\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for Radius')\n",
    "\n",
    "if UNPRED_RATING >= 1 and UNPRED_RATING <= 5:\n",
    "    print('Rating set for unpredicted ratings = {}'. format(UNPRED_RATING))\n",
    "elif UNPRED_RATING == -1:\n",
    "    UNPRED_RATING = 3\n",
    "    print('Rating set for unpredicted ratings = {} (default)'. format(UNPRED_RATING))\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for UNPRED_RATING')\n",
    "    \n",
    "if TRAIN_TEST_SPLIT > 0 and TRAIN_TEST_SPLIT < 1:\n",
    "    print('TRAIN_TEST_SPLIT = {}'.format(TRAIN_TEST_SPLIT))\n",
    "elif TRAIN_TEST_SPLIT == -1:\n",
    "    TRAIN_TEST_SPLIT = 0.2\n",
    "    print('TRAIN_TEST_SPLIT = 0.2 (default)')\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for TRAIN_TEST_SPLIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to read data file, given in CSR format\n",
    "    Assuming 1st 3 values of a row as: user_id, item_id, rating '''\n",
    "def read_data_csr(fname, delimiter, dtype=float):\n",
    "    data_csr = np.loadtxt(fname=fname, delimiter=delimiter, dtype=dtype) # Reading data to array\n",
    "    data_csr = data_csr[:, :3]                                           # Extracting 1st 3 columns: 0,1,2\n",
    "    if FIRST_INDEX == -1:                                                # Making sure user_id/item_id starts from 0\n",
    "        first_index_user = min(data_csr[:,0])                            #    as it becomes easier to track in graphs\n",
    "        first_index_item = min(data_csr[:,1])\n",
    "        data_csr[:,0] = data_csr[:,0] - first_index_user\n",
    "        data_csr[:,1] = data_csr[:,1] - first_index_item\n",
    "    else:\n",
    "        data_csr[:,0:2] = data_csr[:,0:2] - FIRST_INDEX\n",
    "    return data_csr\n",
    "\n",
    "''' Function to get data in CSR format for given data in matrix format '''\n",
    "def matrix_to_csr(data_matrix):\n",
    "    data_csr = np.array([ [i,j,data_matrix[i,j]]\\\n",
    "                         for i in range(len(data_matrix))\\\n",
    "                             for j in range(len(data_matrix[i]))\\\n",
    "                                if data_matrix[i,j] != UNOBSERVED])\n",
    "    return data_csr\n",
    "\n",
    "\n",
    "'''Function to find and replace some values\n",
    "   for only 1d and 2d numpy arrays'''\n",
    "def find_and_replace(data, find_value, replace_value):\n",
    "    if len(data.shape) == 1:                # for 1D numpy array\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == find_value:\n",
    "                data[i] = replace_value\n",
    "    elif len(data.shape) == 2:              # for 2D numpy array\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i,j] == find_value:\n",
    "                    data[i,j] = replace_value\n",
    "    return data\n",
    "\n",
    "''' Function to check dataset'''\n",
    "def check_and_set_data_csr(data_csr):\n",
    "    global USERS, ITEMS, SPARSITY\n",
    "    n_users = int(max(data_csr[:,0])) + 1\n",
    "    n_items = int(max(data_csr[:,1])) + 1\n",
    "    \n",
    "    unique_users = len(np.array(list(set(data_csr[:,0]))))\n",
    "    unique_items = len(np.array(list(set(data_csr[:,1]))))\n",
    "    \n",
    "    if USERS == -1:\n",
    "        USERS = n_users\n",
    "    if ITEMS == -1:\n",
    "        ITEMS = n_items\n",
    "    \n",
    "    print('USERS = ' + str(USERS))\n",
    "    print('ITEMS = ' + str(ITEMS))\n",
    "    \n",
    "    #checking if global USERS/ITEMS had wrong values entered:\n",
    "    if n_users != USERS:\n",
    "        print('ERROR: USERS entered by you is wrong. {} users found in dataset'.format(n_users))\n",
    "    if n_items != ITEMS:\n",
    "        print('ERROR: ITEMS entered by you is wrong. {} items found in dataset'.format(n_items))\n",
    "    \n",
    "    # checking unrated users/items : this is possible if some user/item index gets skipped in dataset\n",
    "    if n_users != unique_users:\n",
    "        print('ERROR: No. of users with no ratings: ' + str(n_users - unique_users))\n",
    "        print('     : This notebook may not be robust to such dataset')\n",
    "    if n_items != unique_items:\n",
    "        print('ERROR: No. of items with no ratings: ' + str(n_items - unique_items))\n",
    "        print('     : This notebook may not be robust to such dataset')\n",
    "    if n_users == unique_users and n_items == unique_items:\n",
    "        print('All users and items have at least one rating! Good!')\n",
    "    \n",
    "    #checking sparsity for large symmetricized matrix\n",
    "    sparsity_symm =  float(2 * N_RATINGS) / ((USERS + ITEMS)**2)\n",
    "    \n",
    "    if SPARSITY == -1:\n",
    "        SPARSITY = sparsity_symm\n",
    "    print('SPARSITY (p) = ' + str(SPARSITY))\n",
    "    if SPARSITY != sparsity_symm:\n",
    "        print('ERROR: SPARSITY entered by you is wrong. {} sparsity found in dataset'.format(sparsity_symm))\n",
    "    \n",
    "    if sparsity_symm <= (float(1) / ((n_users + n_items)**2)):\n",
    "        print('WARNING: For generated large symmetric matrix:')\n",
    "        print('       : p is not polynomially larger than 1/n.')\n",
    "        print('       : Using dist1 as distance computation may not gurantee that')\n",
    "        print('       : expected square error converges to zero using this paper\\'s algorithm.')\n",
    "    else:\n",
    "        print('Sym matrix : p is polynomially larger than 1/n, all guarantees applicable')\n",
    "    print('Check and set dataset : done')\n",
    "\n",
    "'''Function to generate training and testing data split from given CSR data'''\n",
    "def generate_train_test_split_csr(data_csr, split, shuffle=True):\n",
    "    # we use data_csr as it is easy to only shuffle it and accordingly create train and test set\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data_csr) # inplace shuffle\n",
    "\n",
    "    train_sz = int((1 - split) * data_csr.shape[0])\n",
    "\n",
    "    train_data_csr = data_csr[: train_sz ,:]\n",
    "    test_data_csr = data_csr[train_sz : ,:]\n",
    "\n",
    "    if train_data_csr.shape[0]+test_data_csr.shape[0] == data_csr.shape[0]:\n",
    "        print('Generating train test split: done')\n",
    "    else:\n",
    "        print('Generating train test split: FAILED')\n",
    "    return [train_data_csr, test_data_csr]\n",
    "\n",
    "'''Function to force reduce the size of dataset\n",
    "   To be used only for testing purposes\n",
    "   Note: this doesnt ensure if every item has a rating or not: TODO'''\n",
    "def reduce_size_of_data_csr(data_csr):\n",
    "    global N_RATINGS\n",
    "    print('WARNING: FOR TESTING PURPOSES ONLY')\n",
    "    if USER_LIMIT < 1 or ITEM_LIMIT < 1:\n",
    "        print('ERROR: please set limits > 0')\n",
    "        print('     : using same sataset without any reductions')\n",
    "        return data_csr\n",
    "    \n",
    "    data_csr = data_csr[((data_csr[:,0] < USER_LIMIT)*(data_csr[:,1] < ITEM_LIMIT))]\n",
    "    \n",
    "    # Accounting for unvisited users\n",
    "    visited = np.full((USER_LIMIT), True, dtype=bool)\n",
    "    for i in data_csr:\n",
    "        visited[int(i[0])] = False\n",
    "    unvisited_users = [i for i in range(len(visited)) if visited[i]]\n",
    "    # adding 1 rating for every unvisited user\n",
    "    for i in unvisited_users:\n",
    "        data_csr = np.append(data_csr, [[i, i, 3]], axis=0)\n",
    "    \n",
    "    N_RATINGS = data_csr.shape[0]\n",
    "    return data_csr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: FOR TESTING PURPOSES ONLY\n",
      "Reading dataset: done\n",
      "USERS = 200\n",
      "ITEMS = 500\n",
      "All users and items have at least one rating! Good!\n",
      "SPARSITY (p) = 0.0541632653061\n",
      "Sym matrix : p is polynomially larger than 1/n, all guarantees applicable\n",
      "Check and set dataset : done\n"
     ]
    }
   ],
   "source": [
    "data_csr = read_data_csr(fname=DATA_PATH, delimiter=DELIMITER)\n",
    "\n",
    "if SIZE_REDUCTION:\n",
    "    data_csr = reduce_size_of_data_csr(data_csr)\n",
    "\n",
    "if data_csr.shape[0] == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('Reading dataset: done')\n",
    "else:\n",
    "    print('Reading dataset: FAILED')\n",
    "    print( '# of missing ratings: ' + str(N_RATINGS - data_csr.shape[0]))\n",
    "    \n",
    "check_and_set_data_csr(data_csr=data_csr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train test split: done\n"
     ]
    }
   ],
   "source": [
    "[train_data_csr, test_data_csr] = generate_train_test_split_csr(data_csr=data_csr, split=TRAIN_TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I: Model preparation\n",
    "We first look at function which converts our asymmetric rating matrix to a symmetric matrix and another function that normalizes the ratings between [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to normalize the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to normalize all ratings of a CSR (compressed sparse row) matrix'''\n",
    "def normalize_ratings_csr(data_csr):\n",
    "    #TODO: assuming non negative ratings, make it generic\n",
    "    data_csr[:,2] = data_csr[:,2] / float(max(data_csr[:,2]))\n",
    "    print('Normalize ratings: done')\n",
    "    return data_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to make a rating CSR matrix symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function to get data in matrix format for given data in CSR format '''\n",
    "def csr_to_matrix(data_csr, symmetry=False):\n",
    "    \n",
    "    data_matrix = np.full(((USERS+ITEMS), (USERS+ITEMS)), UNOBSERVED, dtype=float)\n",
    "    for line in data_csr:\n",
    "        data_matrix[int(line[0])][int(line[1])] = line[2]\n",
    "        if symmetry:\n",
    "            data_matrix[int(line[1])][int(line[0])] = line[2]\n",
    "            \n",
    "    return data_matrix\n",
    "\n",
    "'''Function get matrix from csr such that no two item_ids and user_ids are same'''\n",
    "def get_csr_with_offset(data_csr, offset):\n",
    "    new_data_csr = np.copy(data_csr)\n",
    "    new_data_csr[:,1] = new_data_csr[:,1] + offset            # so that user_ids != item_ids\n",
    "    #new_data_matrix = csr_to_matrix(new_data_csr, symmetry=True)\n",
    "    return new_data_csr\n",
    "\n",
    "'''MAIN Function to convert asymmetrix CSR matrix to symmetrix matrix\n",
    "   the returned CSR doesnt contain repitions for any user-item pair.\n",
    "   Repetitions can be generated for a 2D matrix by calling csr_to_matrix(data_csr, symmetry=True)'''\n",
    "def csr_to_symmetric_csr(data_csr):\n",
    "    # Assuming the rating matrix to be non symmetric\n",
    "    # Even if it is symmetric, the user_id and item_id need to be different for graph\n",
    "    data_csr = get_csr_with_offset(data_csr, offset=USERS)\n",
    "    print('CSR to symmetric CSR matrix: done')\n",
    "    return data_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize ratings: done\n",
      "CSR to symmetric CSR matrix: done\n"
     ]
    }
   ],
   "source": [
    "train_data_csr = normalize_ratings_csr(train_data_csr)\n",
    "train_data_csr = csr_to_symmetric_csr(train_data_csr)\n",
    "# the symmetric matrix obtained doesnt contain repirions for any user item pair\n",
    "# only the item_ids are scaled by item_ids += USERS\n",
    "# hence, we can safely go ahead and use this CSR matrix for sample splitting step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II: Algorithm Details\n",
    "As per paper: *We present and discuss details of each step of the algorithm, which primarily involves computing pairwise distances (or similarities) between vertices.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Sample Splitting\n",
    "Partition the rating matrix into three different parts. Following are the exerpts from paper:\n",
    "- *Each edge in $E$ is independently placed into $E_1, E_2,$ or $E_3$, with probabilities $c_1, c_2,$ and $1 - c_1 - c_2$ respectively. Matrices $M_1, M_2$, and $M_3$ contain information from the subset of the data in $M$ associated to $E_1, E_2$, and $E_3$ respectively.*\n",
    "- *$M_1$ is used to define local neighborhoods of each vertex (in step 2), $M_2$ is used to compute similarities of these neighborhoods (in step 3), and $M_3$ is used to average over datapoints for the final estimate (in step 4)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_splitting_csr(data_csr, c1=0.33, c2=0.33, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data_csr) # inplace shuffle\n",
    "\n",
    "    m1_sz = int(c1 * data_csr.shape[0])\n",
    "    m2_sz = int(c2 * data_csr.shape[0])\n",
    "\n",
    "    m1_csr = data_csr[              : m1_sz         ,:]\n",
    "    m2_csr = data_csr[        m1_sz : m1_sz + m2_sz ,:]\n",
    "    m3_csr = data_csr[m1_sz + m2_sz :               ,:]\n",
    "\n",
    "    if m1_csr.shape[0]+m2_csr.shape[0]+m3_csr.shape[0] == data_csr.shape[0]:\n",
    "        print('Sample splitting: done')\n",
    "    else:\n",
    "        print('Sample splitting: FAILED')\n",
    "        \n",
    "    return [m1_csr, m2_csr, m3_csr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splitting: done\n"
     ]
    }
   ],
   "source": [
    "[m1_csr, m2_csr, m3_csr] = sample_splitting_csr(data_csr=train_data_csr, c1=C1, c2=C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Expanding the Neighborhood\n",
    "We do the following in this step:\n",
    "- radius $r$ to be tuned using cross validation. We can use its default value as $r = \\frac{6\\ln(1/p)}{8\\ln(c_1pn)}$ as per paper.\n",
    "- use matrix $M_1$ to build neighborhood based on radius $r$\n",
    "- Build BFS tree rooted at each vertex to get product of the path from user to item, such that\n",
    "  - each vertex (user or item) in a path from user to boundary item is unique\n",
    "  - the path chosen is the shortest path (#path edges) between the user and the boundary item\n",
    "  - in case of multiple paths (or trees), choose any one path (i.e. any one tree) at random\n",
    "- Normalize the product of ratings by total no of final items at the boundary\n",
    "\n",
    "$N_{u,r}$ obtained is a vector for user $u$ for $r$-hop, where each element is product of path from user to item or zero. $\\tilde{N_{u,r}}$ is normalized vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing (next two cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np                             ##### REMOVE THIS CELL\n",
    "# from tqdm import *\n",
    "# import sys\n",
    "\n",
    "# FIRST_INDEX = -1\n",
    "# USERS = -1\n",
    "# ITEMS = -1\n",
    "# SPARSITY = -1                  # 'p' in the equations\n",
    "# UNOBSERVED = 0                 # default value in matrix for unobserved ratings\n",
    "# N_RATINGS = 7\n",
    "# C1 = 0                         # only to account for scale_factor in step 3\n",
    "# C2 = 1                         # only to account for scale_factor in step 3\n",
    "\n",
    "# RADIUS = 3                              # radius of neighborhood, radius = # edges between start and end vertex\n",
    "# UNPRED_RATING = -1                      # rating (normalized) for which we dont have predicted rating\n",
    "\n",
    "# m1_csr = read_data_csr(fname='./very_small_graph.txt', delimiter=\"\\t\")\n",
    "# check_and_set_data_csr(data_csr=m1_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1_csr = normalize_ratings_csr(m1_csr)          ##### REMOVE THIS CELL\n",
    "# m1_csr = csr_to_symmetric_csr(m1_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to create a graph as adjacency list: a dictionary of sets'''\n",
    "def create_dict_graph_from_csr(data_csr):\n",
    "    data_matrix = csr_to_matrix(data_csr, symmetry=True)\n",
    "    # Create an (unweighted) adjacency list for the graph\n",
    "    ## we still have the 2D matrix for the weights\n",
    "    graph = dict()\n",
    "    print('Creating graph as dictionary:')\n",
    "    sys.stdout.flush()\n",
    "    for i in tqdm(range(len(data_matrix))):\n",
    "        temp_set = set()\n",
    "        for j in range(len(data_matrix[i])):\n",
    "            if data_matrix[i,j] > 0:\n",
    "                temp_set.add(j)\n",
    "        graph[i] = temp_set\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions useful for getting products along paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''Function gives all possible path from 'start' vertex at r-hop distance '''\n",
    "# help from:\n",
    "# http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/\n",
    "# radius = # edges between start and end vertex\n",
    "def bfs_paths(graph, start, radius):\n",
    "    queue = [(start, [start])]\n",
    "    visited = [start]\n",
    "    while queue:\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        for next in graph[vertex] - set(path):\n",
    "            if next in visited:\n",
    "                continue\n",
    "            depth = len(path + [next]) - 1\n",
    "            if depth == radius:\n",
    "                # We do not append next to visited because\n",
    "                # we want all shorted paths to next and then\n",
    "                # choose one path at random in get_product()\n",
    "                yield path + [next]\n",
    "            else:\n",
    "                queue.append((next, path + [next]))\n",
    "                visited.append(next)\n",
    "\n",
    "'''Function which returns a dictionary for a given user\n",
    "   where each item represents the key in the dictionary\n",
    "   and it returns a list of lists(paths) from user to item r-hop distance apart'''\n",
    "def create_item_dict(all_path):\n",
    "    dict_path = dict()\n",
    "    for path in all_path:\n",
    "        r_hop_item = path[-1]\n",
    "        dict_path.setdefault(r_hop_item,[]).append(path) \n",
    "    return dict_path\n",
    "\n",
    "'''Function to get product from user to item in the path\n",
    "   It chooses any path at random, if #paths > 1'''\n",
    "def get_product(data_matrix, path):\n",
    "    if len(path) < 1:\n",
    "        return UNOBSERVED\n",
    "    idx = random.randint(0, len(path)-1)    # in case of multiple paths to same item\n",
    "    p = path[idx]                           # choose any one path at random\n",
    "\n",
    "    product = 1\n",
    "    for i in range(len(p)-1):\n",
    "        product = product * data_matrix[p[i],p[i+1]]\n",
    "    return product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to generate product matrix from user to items\n",
    "   (items which are at r-hop boundary from user)'''\n",
    "def generate_product_matrix(graph, data_matrix, radius):\n",
    "    \n",
    "    # each u'th row in product_matrix represents a neighbor boundary vector for vertex u\n",
    "    # therefore it is a (USERS+ITEMS) x (USERS+ITEMS) dimensional matrix\n",
    "    product_matrix = np.full(((USERS+ITEMS), (USERS+ITEMS)), UNOBSERVED, dtype=float)\n",
    "    \n",
    "    for user_vertex in tqdm(range(USERS+ITEMS)):               #a user_vertex may also be of an item\n",
    "        all_path = list(bfs_paths(graph, user_vertex, radius)) # 1. get a list of all r-hop paths from given user\n",
    "        dict_path = create_item_dict(all_path)                 # 2. create dict of paths from user to individual items\n",
    "        for item_vertex in dict_path:                          #an item_vertex may also be of a user\n",
    "            paths = dict_path[item_vertex]                     # 3. get the set of user-item paths\n",
    "            product = get_product(data_matrix, paths)          # 4. get product for a unique user-item path (at random)\n",
    "            product_matrix[user_vertex, item_vertex] = product # 5. store the product in the matrix\n",
    "    return product_matrix\n",
    "\n",
    "'''Function to normalize the product of paths in neighbor boundary vector for every u'th rowed user\n",
    "   normalized along the same row'''\n",
    "#TODO: implement it in efficient manner\n",
    "def row_wise_normalize_matrix(data_matrix):\n",
    "    n_neighbors_per_row = np.full((USERS+ITEMS), 0, dtype=float)\n",
    "    for i in range(len(data_matrix)):\n",
    "        for j in range(len(data_matrix[i])):\n",
    "            if data_matrix[i,j] != UNOBSERVED:\n",
    "                n_neighbors_per_row[i] = n_neighbors_per_row[i] + 1\n",
    "    \n",
    "    for i in range(len(data_matrix)):\n",
    "        for j in range(len(data_matrix[i])):\n",
    "            if data_matrix[i,j] != UNOBSERVED and n_neighbors_per_row[i] > 0:\n",
    "                data_matrix[i,j] = data_matrix[i,j] / n_neighbors_per_row[i]\n",
    "    \n",
    "    return data_matrix\n",
    "\n",
    "import pandas as pd\n",
    "'''Function to describe count of neighbors for every vertex and other values\n",
    "   Also note, the values described might be slightly distorted because of symmetricity of neighbor matrices'''\n",
    "def describe_neighbor_count(data_matrix):\n",
    "    n_neighbor_vector = np.full((USERS+ITEMS), 0, dtype=float)\n",
    "    for i in range(len(data_matrix)):\n",
    "        for j in range(len(data_matrix[i])):\n",
    "            if data_matrix[i,j] != UNOBSERVED:\n",
    "                n_neighbor_vector[i] = n_neighbor_vector[i] + 1\n",
    "    \n",
    "    df = pd.DataFrame(n_neighbor_vector)\n",
    "    print('To effectively choose RADIUS value for next run of algorithm:')\n",
    "    print('Showing distribution of count of neighbors for every vertex:')\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "'''Function to return two product matrices\n",
    "   one at r-hop distance and other at r+1 hop distance for dist1 computation'''\n",
    "# if radius passed is less than 1 or not passed, this function evaluates the default radius as per paper\n",
    "def generate_neighbor_boundary_matrix(data_csr):\n",
    "    global RADIUS\n",
    "    if RADIUS < 1:\n",
    "        #TODO: Fix this\n",
    "        print('ERROR: please do not use the radius formula as given in paper')\n",
    "        print('     : the formula evaluates to a decimal values between 0 and 1')\n",
    "        print('     : I am working on fixing this')\n",
    "        RADIUS = (float(6) * math.log( 1.0 / SPARSITY)) / (8.0 * math.log( float(C1) * SPARSITY * (USERS + ITEMS)))\n",
    "        return -1\n",
    "    \n",
    "    # First create the graph\n",
    "    graph = create_dict_graph_from_csr(data_csr)              # to store the edges in adjacency list\n",
    "    data_matrix = csr_to_matrix(data_csr, symmetry=True)      # to store the ratings as matrix\n",
    "    \n",
    "    radius = RADIUS\n",
    "    print('Generating neighbor boundary matrix at {}-hop distance:'.format(radius))\n",
    "    sys.stdout.flush()\n",
    "    r_neighbor_matrix = generate_product_matrix(graph, data_matrix, radius=radius)\n",
    "    r_neighbor_matrix = row_wise_normalize_matrix(r_neighbor_matrix)\n",
    "    \n",
    "    radius = radius+1\n",
    "    print('Generating neighbor boundary matrix at {}-hop distance:'.format(radius))\n",
    "    sys.stdout.flush()\n",
    "    r1_neighbor_matrix = generate_product_matrix(graph, data_matrix, radius=radius)\n",
    "    r1_neighbor_matrix = row_wise_normalize_matrix(r1_neighbor_matrix)\n",
    "    \n",
    "    return [r_neighbor_matrix, r1_neighbor_matrix]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph as dictionary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 1693.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbor boundary matrix at 3-hop distance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 700/700 [00:09<00:00, 72.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbor boundary matrix at 4-hop distance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:23<00:00, 30.25it/s]\n"
     ]
    }
   ],
   "source": [
    "[r_neighbor_matrix, r1_neighbor_matrix] = generate_neighbor_boundary_matrix(m1_csr)\n",
    "# all neighbor boundary vector for each user u is stored as u'th row in neighbor_matrix\n",
    "# though here the vector is stored a row vector, we will treat it as column vector in Step 4\n",
    "# Note: we might expect neighbor matrix to be symmetric with dimensions (USERS+ITEMS)*(USERS+ITEMS)\n",
    "#     : since distance user-item and item-user should be same\n",
    "#     : but this is not the case since there might be multiple paths between user-item\n",
    "#     : and the random path picked for user-item and item-user may not be same\n",
    "#     : normalizing the matrix also will result to rise of difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively choose RADIUS value for next run of algorithm:\n",
      "Showing distribution of count of neighbors for every vertex:\n",
      "                0\n",
      "count  700.000000\n",
      "mean   233.814286\n",
      "std    120.662287\n",
      "min      0.000000\n",
      "25%    176.000000\n",
      "50%    184.000000\n",
      "75%    371.000000\n",
      "max    452.000000\n"
     ]
    }
   ],
   "source": [
    "describe_neighbor_count(r_neighbor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively choose RADIUS value for next run of algorithm:\n",
      "Showing distribution of count of neighbors for every vertex:\n",
      "                0\n",
      "count  700.000000\n",
      "mean   238.168571\n",
      "std    124.431055\n",
      "min      0.000000\n",
      "25%    131.750000\n",
      "50%    256.000000\n",
      "75%    345.000000\n",
      "max    445.000000\n"
     ]
    }
   ],
   "source": [
    "describe_neighbor_count(r1_neighbor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Computing the distances\n",
    "Distance computation between two users (using matrix $M_2$) using the following formula (only $dist_1$ implemented for now):\n",
    "\n",
    "$$ dist(u,v) = \\left(\\frac{1 - c_1p}{c_2p}\\right) (\\tilde{N_{u,r}} - \\tilde{N_{v,r}})^T M_2 (\\tilde{N_{u,r+1}} - \\tilde{N_{v,r+1}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(r_neighbor_matrix, r1_neighbor_matrix, m2_csr):\n",
    "    m2_matrix = csr_to_matrix(m2_csr, symmetry=True)\n",
    "    scale_factor = (1.0 - C1 * SPARSITY) / (C2 * SPARSITY)\n",
    "    \n",
    "    user_list = np.array(range(USERS+ITEMS))\n",
    "    distance_matrix = np.full(((USERS+ITEMS), (USERS+ITEMS)), UNOBSERVED, dtype=float)\n",
    "    \n",
    "    print('Generating distance matrix')\n",
    "    sys.stdout.flush()\n",
    "    for user1 in tqdm(user_list):  # computing for all elements individually\n",
    "        for user2 in user_list:    # not assuming any symmetricity for distance matrix\n",
    "            user1_r_neighbor_vector = r_neighbor_matrix[user1]\n",
    "            user2_r_neighbor_vector = r_neighbor_matrix[user2]\n",
    "            r_neighbor_vector = user1_r_neighbor_vector - user2_r_neighbor_vector\n",
    "            r_neighbor_vector = np.transpose(r_neighbor_vector)\n",
    "\n",
    "            user1_r1_neighbor_vector = r1_neighbor_matrix[user1]\n",
    "            user2_r1_neighbor_vector = r1_neighbor_matrix[user2]\n",
    "            r1_neighbor_vector = user1_r1_neighbor_vector - user2_r1_neighbor_vector\n",
    "\n",
    "            #print(r_neighbor_vector.shape)\n",
    "            #print(m2_matrix.shape)\n",
    "            dist_value = np.matmul(r_neighbor_vector, m2_matrix)\n",
    "            dist_value = np.matmul(dist_value, r1_neighbor_vector)\n",
    "\n",
    "            distance_matrix[user1,user2] = dist_value * scale_factor\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "import pandas as pd\n",
    "'''Function which gives an idea of how distance values are distributed to best choose THRESHOLD in Step 4'''\n",
    "def describe_distance_matrix(distance_matrix):\n",
    "    flat_distance_matrix = distance_matrix.flatten()\n",
    "    observed = []\n",
    "    for i in flat_distance_matrix:\n",
    "        if i != UNOBSERVED:\n",
    "            observed.append(i)\n",
    "\n",
    "    df = pd.DataFrame(observed)\n",
    "    print('To effectively choose THRESHOLD value in next step:')\n",
    "    print('Showing distribution of non zero (or observed) entries of distance matrix:')\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating distance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [02:07<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "distance_matrix = compute_distance_matrix(r_neighbor_matrix, r1_neighbor_matrix, m2_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively choose THRESHOLD value in next step:\n",
      "Showing distribution of non zero (or observed) entries of distance matrix:\n",
      "                   0\n",
      "count  488430.000000\n",
      "mean        0.149607\n",
      "std         0.168090\n",
      "min        -0.095719\n",
      "25%         0.005954\n",
      "50%         0.076988\n",
      "75%         0.272288\n",
      "max         1.043911\n"
     ]
    }
   ],
   "source": [
    "describe_distance_matrix(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_matrix = compute_distance_matrix(r_neighbor_matrix, r1_neighbor_matrix, m1_csr)\n",
    "# distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe_distance_matrix(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Averaging datapoints to produce final estimate\n",
    "Average over nearby data points based on the distance(similarity) threshold $n_n$ (using matrix $M_3$). $n_n$ to be tuned using cross validation. Mathematically (from paper):\n",
    "\n",
    "$$ \\hat{F_{u,v}} = \\frac{1}{\\mid E_{uv1} \\mid} \\sum_{(a,b) \\in E_{uv1}} M_3(a,b) $$\n",
    "*where $E_{uv1}$ denotes the set of undirected edges $(a, b)$ such that $(a, b) \\in E_3$ and both $dist(u, a)$ and $dist(v, b)$ are less than $n_n$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to get similarity matrix using THRESHOLD'''\n",
    "def generate_sim_matrix(distance_matrix, threshold):\n",
    "    user_list = np.array(range(USERS+ITEMS))\n",
    "    sim_matrix = np.full(((USERS+ITEMS), (USERS+ITEMS)), False, dtype=bool)\n",
    "    \n",
    "    print('Generating distance similarity matrix:')\n",
    "    sys.stdout.flush()\n",
    "    for user1 in tqdm(user_list):  # computing for all elements individually\n",
    "        for user2 in user_list:    # not assuming any symmetricity for distance matrix\n",
    "            if distance_matrix[user1,user2] != UNOBSERVED and distance_matrix[user1,user2] < threshold:\n",
    "                sim_matrix[user1, user2] = True\n",
    "    return sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to get final prediction estimates for user-item ratings'''\n",
    "def generate_averaged_prediction(u, v, sim_matrix, m3_matrix, bounded=True):\n",
    "    prediction = 0\n",
    "    n_prediction = 0\n",
    "    \n",
    "    # Making sure the vertex indices are ints\n",
    "    u = int(u)\n",
    "    v = int(v)\n",
    "    \n",
    "    for a in range(len(sim_matrix[u])):\n",
    "        for b in range(len(sim_matrix[v])):\n",
    "            if sim_matrix[u,a] and sim_matrix[v,b] and m3_matrix[a,b] != UNOBSERVED:\n",
    "                prediction = prediction + m3_matrix[a,b]\n",
    "                n_prediction = n_prediction + 1\n",
    "    if n_prediction > 0:    #TODO: write why they are present\n",
    "        prediction = prediction / n_prediction\n",
    "    else:\n",
    "        prediction = UNPRED_RATING / 5                       # TODO: make it generic; here we assume ratings as 1 - 5\n",
    "        \n",
    "    if bounded:                                              #     : make it generic; here we assume ratings as 1 - 5\n",
    "        if prediction > 1:\n",
    "            prediction = 1\n",
    "        elif prediction < 0.2:\n",
    "            prediction = 0.2\n",
    "\n",
    "    return prediction\n",
    "\n",
    "'''Function to get final prediction estimates for user-item rating matrix\n",
    "   Use this only if you want estimates for all the ratings'''\n",
    "def generate_averaged_prediction_matrix(sim_matrix, m3_csr):\n",
    "    m3_matrix = csr_to_matrix(m3_csr, symmetry=True)\n",
    "    \n",
    "    vertex_list = np.array(range(USERS+ITEMS))\n",
    "    prediction_matrix = np.full(((USERS+ITEMS), (USERS+ITEMS)), UNOBSERVED, dtype=float)\n",
    "    \n",
    "    print('Generating prediction matrix:')\n",
    "    sys.stdout.flush()\n",
    "    for u in tqdm(vertex_list):\n",
    "        for v in vertex_list:\n",
    "            prediction = generate_averaged_prediction(u, v, sim_matrix, m3_matrix, bounded=True)\n",
    "            prediction_matrix[u, v] = prediction\n",
    "    \n",
    "    return prediction_matrix\n",
    "\n",
    "'''Function to get final prediction estimates for given user-item list\n",
    "   Use this only if you have a set of user-item pairs for which you want the final estimates\n",
    "   Consider using this function for evaluation purposes(only)'''\n",
    "def generate_averaged_prediction_array(sim_matrix, m3_csr, test_data_csr):\n",
    "    m3_matrix = csr_to_matrix(m3_csr, symmetry=True)\n",
    "    \n",
    "    prediction_array = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "    \n",
    "    print('Generating prediction array:')\n",
    "    sys.stdout.flush()\n",
    "    for i in tqdm(range(len(test_data_csr))):\n",
    "        datapt = test_data_csr[i]\n",
    "        # Considering only first two columns for rating estimation\n",
    "        vertex1 = int(datapt[0])\n",
    "        vertex2 = int(datapt[1])\n",
    "        prediction = generate_averaged_prediction(vertex1, vertex2, sim_matrix, m3_matrix, bounded=True)\n",
    "        prediction_array[i] = prediction\n",
    "    return prediction_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating distance similarity matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 718.37it/s]\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 2            # prefer to choose a threshold now if not chosen in initial Hyperparameter decision stage\n",
    "sim_matrix = generate_sim_matrix(distance_matrix, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize ratings: done\n",
      "CSR to symmetric CSR matrix: done\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test dataset using Model preparation section functions\n",
    "test_data_csr = normalize_ratings_csr(test_data_csr)\n",
    "test_data_csr = csr_to_symmetric_csr(test_data_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction array:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2654/2654 [27:17<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting estimates for only test data points\n",
    "prediction_array = generate_averaged_prediction_array(sim_matrix, m3_csr, test_data_csr)\n",
    "\n",
    "# To generate complete rating matrix do the following:\n",
    "#prediction_matrix = generate_averaged_prediction_matrix(sim_matrix, m3_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_matrix = generate_sim_matrix(distance_matrix, threshold=.26)\n",
    "# sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_array = generate_averaged_prediction_array(sim_matrix, m1_csr, m1_csr)\n",
    "# prediction_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_matrix = generate_averaged_prediction_matrix(sim_matrix, m1_csr)\n",
    "# prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We evaluate our recommendation algorithm using RMSE (root mean square error). <br>\n",
    "According to paper, if sparsity $p$ is polynomially larger than $n^{-1}$, i.e. if $p = n^{-1 + \\epsilon}$ for $\\epsilon > 0$, then we can safely use $dist_1$ distance computation formula and MSE is bounded by $O((pn)^{-1/5})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#TODO: describe these functions\n",
    "'''Function to generate true and test labels from test_data_csr and predicted_matrix\n",
    "   This function may not be required for evaluation purposes(only)'''\n",
    "def generate_true_and_test_labels(test_data_csr, predicted_matrix):\n",
    "    # for all the available ratings in testset\n",
    "    # and for all the predicted rating for those available rating\n",
    "    # put them in two separate vectors\n",
    "    y_actual  = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "    y_predict = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "\n",
    "    print('Generating true and test label:')\n",
    "    sys.stdout.flush()\n",
    "    for i in tqdm(range(len(test_data_csr))):\n",
    "        testpt = test_data_csr[i]\n",
    "        y_actual[i]  = testpt[2]\n",
    "        y_predict[i] = predicted_matrix[testpt[0], testpt[1]]\n",
    "        if y_predict[i] == UNOBSERVED:       # i.e. we could not generate a rating for this test user item pair\n",
    "            y_predict[i] = AVG_RATING\n",
    "    return [y_actual, y_predict]\n",
    "\n",
    "'''Function to get Mean Squared Error for given actual and predicted array'''\n",
    "def get_mse(y_actual, y_predict):\n",
    "    mse = mean_squared_error(y_actual, y_predict)\n",
    "    return mse\n",
    "\n",
    "'''Function to get ROOT Mean Squared Error for given actual and predicted array'''\n",
    "def get_rmse(y_actual, y_predict):\n",
    "    rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
    "    return rmse\n",
    "    \n",
    "'''Function to get Average Error for given actual and predicted array'''\n",
    "def get_avg_err(y_actual, y_predict):\n",
    "    avg_err = sum(abs(y_actual - y_predict)) / len(y_actual)\n",
    "    return avg_err\n",
    "\n",
    "'''Function to check if obtained MSE is within the bound as calculated in the paper'''\n",
    "def check_mse(data_csr, y_actual, y_predict):\n",
    "    mse_upper_bound = (SPARSITY * (USERS+ITEMS)) ** (-1 / float(5))\n",
    "    print('MSE Upper bound: {}'.format(mse_upper_bound))\n",
    "    mse = get_mse(y_actual, y_predict)\n",
    "    print('MSE of predictions: {}'.format(mse))\n",
    "    if mse < mse_upper_bound:\n",
    "        print('As per the discussion in the paper, MSE is bounded by O((pn)**(-1/5))')\n",
    "    else:\n",
    "        print('ERROR: Contrary to the discusssion in the paper, MSE is NOT bounded by O((pn)**(-1/5))')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already prepared the test data (required for our algorithm)\n",
    "y_actual  = test_data_csr[:,2]\n",
    "y_predict = prediction_array\n",
    "# If we want, we could scale our ratings back to 1 - 5 range for evaluation purposes\n",
    "#But then paper makes no guarantees about scaled ratings\n",
    "#y_actual  = y_actual * 5\n",
    "#y_predict = y_predict * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22815160919895225"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rmse(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18737974456239831"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_err(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Upper bound: 0.483325183274\n",
      "MSE of predictions: 0.0520531567801\n",
      "As per the discussion in the paper, MSE is bounded by O((pn)**(-1/5))\n"
     ]
    }
   ],
   "source": [
    "check_mse(data_csr, y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling and doing the same checks\n",
    "#But then paper makes no guarantees about scaled ratings\n",
    "y_actual  = y_actual * 5\n",
    "y_predict = y_predict * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1407580459947613"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rmse(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93689872281199293"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_err(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Upper bound: 0.483325183274\n",
      "MSE of predictions: 1.3013289195\n",
      "ERROR: Contrary to the discusssion in the paper, MSE is NOT bounded by O((pn)**(-1/5))\n"
     ]
    }
   ],
   "source": [
    "check_mse(data_csr, y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We have already prepared the test data (required for our algorithm)\n",
    "# test_data_csr = m1_csr\n",
    "# y_actual  = test_data_csr[:,2]\n",
    "# y_predict = prediction_array\n",
    "# # If we want, we could scale our ratings back to 1 - 5 range for evaluation purposes\n",
    "# #But then paper makes no guarantees about scaled ratings\n",
    "# #y_actual  = y_actual * 5\n",
    "# #y_predict = y_actual * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rmse(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_err(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_mse(m1_csr, y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Scaling and doing the same checks\n",
    "# #But then paper makes no guarantees about scaled ratings\n",
    "# y_actual  = y_actual * 5\n",
    "# y_predict = y_predict * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rmse(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_err(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_mse(m1_csr, y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(17, 14, 14, 405545)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().time()     # (hour, min, sec, microsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
