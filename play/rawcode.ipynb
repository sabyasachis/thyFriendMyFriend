{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Built on python2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './ml-100k/u.data' # ml-100k data set has 100k ratings, 943 users and 1682 items\n",
    "\n",
    "# Dataset Parameters\n",
    "DATA_TYPE =  0              # 0: CSR format, 1: 2D matrix format\n",
    "DELIMITER = \"\\t\"            # tab separated or comma separated data format\n",
    "N_RATINGS = 100000\n",
    "USERS = 943\n",
    "ITEMS = 1682\n",
    "FIRST_INDEX = 1             # index starts from 0 or 1\n",
    "\n",
    "# Hardcoding values\n",
    "UNOBSERVED = -1\n",
    "GET_PRODUCT_FAIL_RETURN = UNOBSERVED    #TODO: This hardcoding can be removed in future\n",
    "OFFSET = USERS + 10     # keep it >= #USERS\n",
    "TRAIN_TEST_SPLIT = 0.2  # %age of test ratings wrt train rating ; value in between 0 and 1\n",
    "AVG_RATING = 3 .        # ratings for which we dont have predicted rating\n",
    "\n",
    "# HyperParameters\n",
    "C1 = 0.2                # probability of edges in training set going to E1\n",
    "C2 = 0.3                # probability of edges in training set going to E2\n",
    "C3 = 1 - C1 - C2\n",
    "RADIUS = 0              # radius of neighborhood, r = 1 implies movies, which were rated by 1 hop friend users...\n",
    "THRESHOLD =  USERS      # distance/similarity threshold for users, who are used in avg weighted estimate computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#checks on parameters set in previous cell\n",
    "if C3 <= 0:\n",
    "    print('ERROR: Please set the values of C1 and C2, s.t, C1+C2 < 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for data handling\n",
    "\n",
    "(need to handle for boundary cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function to get data in matrix format for given data in CSR format '''\n",
    "def csr_to_matrix(data_csr, symmetry=False):\n",
    "    users = max(data_csr[:,0]) + 1\n",
    "    items = max(data_csr[:,1]) + 1\n",
    "    if symmetry:\n",
    "        users = items = max(users,items)\n",
    "        \n",
    "    data_matrix = np.full((users, items), UNOBSERVED, dtype=int)\n",
    "    for line in data_csr:\n",
    "        data_matrix[line[0]][line[1]] = line[2]\n",
    "        if symmetry:\n",
    "            data_matrix[line[1]][line[0]] = line[2]\n",
    "            \n",
    "    return data_matrix\n",
    "\n",
    "''' Function to get data in CSR format for given data in matrix format '''\n",
    "def matrix_to_csr(data_matrix):\n",
    "    data_matrix = np.empty([0,0], dtype=int)\n",
    "    data_csr = np.array([ [i,j,data_matrix[i,j]]\\\n",
    "                          for j in range(len(temp[i]))\\\n",
    "                              for i in range(len(temp))\\\n",
    "                                  if temp[i,j] > 0])\n",
    "    return data_csr\n",
    "\n",
    "''' Function to read data file, given in CSR format; Assuming 1st 3 values as: user_id, item_id, rating '''\n",
    "def read_data_csr(fname=DATA_PATH, delimiter=DELIMITER, dtype=int):\n",
    "    data_csr = np.loadtxt(fname=fname, delimiter=delimiter, dtype=dtype) # Reading data to array\n",
    "    data_csr = data_csr[:, :3]                                           # Extracting 1st 3 columns: 0,1,2\n",
    "    data_csr[:,:2] = data_csr[:,0:2] - FIRST_INDEX                       # Making sure index starts from 0\n",
    "    return data_csr\n",
    "\n",
    "''' Function to read CSR data file in matrix format '''\n",
    "# to be removed in future\n",
    "import csv\n",
    "def read_csr_data_as_matrix(users=USERS, items=ITEMS, symmetry=False):\n",
    "    if symmetry:\n",
    "        users = items = max(users, items)\n",
    "    data_matrix = np.full((users, items), UNOBSERVED, dtype=int)\n",
    "    with open(DATA_PATH) as tsv:\n",
    "        for line in csv.reader(tsv, delimiter=\"\\t\"):    # line is a list containing all tab separated items\n",
    "            line = [int(i) for i in line]               # convert the list of str to list of int\n",
    "            data_matrix[line[0] - FIRST_INDEX][line[1] - FIRST_INDEX] = line[2]\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "''' Function to read data file, given in matrix format '''\n",
    "# TODO\n",
    "def read_data_matrix():\n",
    "    data_matrix = np.empty([0,0], dtype=int)\n",
    "    return data_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_csr = read_data_csr()\n",
    "\n",
    "if data_csr.shape[0] == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('done')\n",
    "else:\n",
    "    print('fail')\n",
    "    #print( '# of missing ratings: ' + str(N_RATINGS - data_csr.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a train/test split for non negative ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# we use data_csr as it is easy to only shuffle it and accordingly create train and test set\n",
    "np.random.shuffle(data_csr) # inplace shuffle\n",
    "\n",
    "train_sz = int((1 - TRAIN_TEST_SPLIT) * data_csr.shape[0])\n",
    "\n",
    "train_data_csr = data_csr[: train_sz ,:]\n",
    "test_data_csr = data_csr[train_sz : ,:]\n",
    "\n",
    "if train_data_csr.shape[0]+test_data_csr.shape[0] == data_csr.shape[0]:\n",
    "    print('done')\n",
    "else:\n",
    "    print('fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore/Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some data analysis nos and plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Begins from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Sample Splitting\n",
    "\n",
    "Split the matrix edges into 3 parts, M1, M2, M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Again using data_csr is easier to split sample into 3 parts\n",
    "\n",
    "#reshuffling training data can be avoided as it was obtained shuffled already\n",
    "#np.random.shuffle(train_data_csr) # inplace shuffle\n",
    "\n",
    "m1_sz = int(C1 * train_data_csr.shape[0])\n",
    "m2_sz = int(C2 * train_data_csr.shape[0])\n",
    "\n",
    "m1_csr = train_data_csr[              : m1_sz         ,:]\n",
    "m2_csr = train_data_csr[        m1_sz : m1_sz + m2_sz ,:]\n",
    "m3_csr = train_data_csr[m1_sz + m2_sz :               ,:]\n",
    "\n",
    "if m1_csr.shape[0]+m2_csr.shape[0]+m3_csr.shape[0] == train_data_csr.shape[0]:\n",
    "    print('done')\n",
    "else:\n",
    "    print('fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Expanding the neighborhood\n",
    "\n",
    "use M1 to build neighborhood based on radius *r*\n",
    "\n",
    "Normalize the product of ratings by total no of final movies at the boundary\n",
    "\n",
    "Building BFS tree rooted at each vertex, s/t\n",
    "- each node in a path from user to boundary item is unique\n",
    "- shortest path (#path edges) between user and boundary item\n",
    "- in case of multiple paths (or trees) choose any one path (i.e. any one tree) at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m1_csr = read_data_csr(fname='./very_small_graph.txt')   ##### REMOVE THIS CELL\n",
    "#OFFSET = 4 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Create adjacency list: dictionary of sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Step 2.1.1 Convert M1 from csr to matrix format'''\n",
    "# item_ids += OFFSET\n",
    "# so that user_ids != item_ids\n",
    "# and we can create an undirected graph (important to get an edge from item to user)\n",
    "\n",
    "new_m1_csr = np.copy(m1_csr)\n",
    "#new_m1_csr = new_m1_csr + 1                                ##### REMOVE THIS LINE\n",
    "new_m1_csr[:,1] = new_m1_csr[:,1] + OFFSET\n",
    "new_m1_matrix = csr_to_matrix(new_m1_csr, symmetry=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''Step 2.1.2 Create an (unweighted) adjacency list for the graph'''\n",
    "# we still have the 2D matrix for the weights\n",
    "\n",
    "graph = dict()\n",
    "for i in range(len(new_m1_matrix)):\n",
    "    temp_set = set()\n",
    "    for j in range(len(new_m1_matrix[i])):\n",
    "        if new_m1_matrix[i,j] > 0:\n",
    "            temp_set.add(j)\n",
    "    graph[i] = temp_set\n",
    "\n",
    "print('done')  # TODO : put a check for fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: set(),\n",
       " 1: {11, 12},\n",
       " 2: {11},\n",
       " 3: {11, 13},\n",
       " 4: {12, 13},\n",
       " 5: set(),\n",
       " 6: set(),\n",
       " 7: set(),\n",
       " 8: set(),\n",
       " 9: set(),\n",
       " 10: set(),\n",
       " 11: {1, 2, 3},\n",
       " 12: {1, 4},\n",
       " 13: {3, 4}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if 0 in graph:                                              ##### REMOVE THIS CELL\n",
    "#    del graph[0]\n",
    "    \n",
    "#graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: For all users: Get paths for all movies\n",
    "\n",
    "(maybe only unrated movies: TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This implementation for this step is inspired from:\n",
    "# http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/\n",
    "\n",
    "import random\n",
    "\n",
    "'''Step 2.2.1: Function gives all possible path from 'start' vertex to 'goal' vertex, inclusive of both '''\n",
    "def bfs_paths(graph, start, goal):\n",
    "    queue = [(start, [start])]\n",
    "    while queue:\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        for next in graph[vertex] - set(path):\n",
    "            if next == goal:\n",
    "                yield path + [next]\n",
    "            else:\n",
    "                queue.append((next, path + [next]))\n",
    "\n",
    "'''Step 2.2.2: Function to get paths from user to item at r-hop distance'''\n",
    "# assuming self rated movies are at radius = 0 and direct friends' movie at radius = 2\n",
    "def get_r_hop_path(path, radius=RADIUS):\n",
    "    exact_path = []\n",
    "    for p in path:\n",
    "        if ((len(p)/2) - 1) == radius:\n",
    "            exact_path.append(p)\n",
    "    return exact_path\n",
    "\n",
    "'''Step 2.2.3: Function to get product for r-hop path from user to item\n",
    "   Choose any path at random if #paths > 1'''\n",
    "def get_product(path):\n",
    "    if len(path) < 1:\n",
    "        return GET_PRODUCT_FAIL_RETURN\n",
    "    idx = random.randint(0, len(path)-1)    # in case of multiple paths to same item\n",
    "    p = path[idx]                           # choose any one path at random\n",
    "\n",
    "    product = 1\n",
    "    for i in range(len(p)-1):\n",
    "        product = product * new_m1_matrix[p[i],p[i+1]]\n",
    "\n",
    "    return product\n",
    "\n",
    "'''Function to find and replace some values ; for only 1d and 2d numpy arrays'''\n",
    "def find_and_replace(data, find_value, replace_value):\n",
    "    if len(data.shape) == 1:\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == find_value:\n",
    "                data[i] = replace_value\n",
    "    elif len(data.shape) == 2:\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i,j] == find_value:\n",
    "                    data[i,j] = replace_value\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = np.array(list(set(new_m1_csr[:,0])))\n",
    "item_list = np.array(list(set(new_m1_csr[:,1])))\n",
    "\n",
    "product_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=int)\n",
    "\n",
    "for user in user_list:\n",
    "    for item in item_list:\n",
    "        path = list(bfs_paths(graph, user, item))\n",
    "        path = get_r_hop_path(path, radius=1)\n",
    "        predicted_matrix[user, (item - OFFSET)] = get_product(path)       ##### UNCOMMENT THIS LINE\n",
    "        #product_matrix[user-1, (item - OFFSET)-1] = get_product(path)    ##### REMOVE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Computing the distances\n",
    "\n",
    "use M2 to get distance (or similarity) between two users based on neighbourhood from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all unobserved entries in product_matrix as zero\n",
    "# makes it simpler for pearson similarity calculation, probably..\n",
    "\n",
    "product_matrix = find_and_replace(data=product_matrix, find_value=UNOBSERVED, replace_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,  20],\n",
       "       [  0,  20, 100],\n",
       "       [  0,  32,   0],\n",
       "       [ 40,   0,   0]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently using simple pearson similarity:\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "user_sim_matrix = np.full((len(user_list), len(user_list)), UNOBSERVED, dtype=float)\n",
    "for user1 in user_list:\n",
    "    #user1 = user1 - 1                         ##### REMOVE THIS LINE\n",
    "    for user2 in user_list:\n",
    "        #user2 = user2 - 1                         ##### REMOVE THIS LINE\n",
    "        if user1 >= user2:\n",
    "            [sim, p_value] = stats.pearsonr(product_matrix[user1], product_matrix[user2])\n",
    "            user_sim_matrix[user1,user2] = user_sim_matrix[user2,user1] = sim #abs(sim) TODO, check\n",
    "            \n",
    "            # similarity is between 0 and 1\n",
    "            # therefore, these can be directly used as weights on users' rating for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.98198051,  0.5       ,  0.5       ],\n",
       "       [ 0.98198051,  1.        ,  0.32732684,  0.65465367],\n",
       "       [ 0.5       ,  0.32732684,  1.        ,  0.5       ],\n",
       "       [ 0.5       ,  0.65465367,  0.5       ,  1.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Averaging datapts to get final estimates\n",
    "\n",
    "use M3 to perform a weighted avg using similarity computed in previous step ; account for threshold n_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 0],\n",
       "       [4, 0, 0],\n",
       "       [4, 0, 3],\n",
       "       [0, 3, 1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_matrix = csr_to_matrix(m1_csr)\n",
    "m1_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making all unobserved entries in product_matrix as zero\n",
    "# makes it simpler for weighted dot product for rating prediction\n",
    "\n",
    "m1_matrix = find_and_replace(data=m1_matrix, find_value=UNOBSERVED, replace_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "predicted_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=float)\n",
    "for user in user_list:\n",
    "    #user = user - 1                                     ##### REMOVE THIS LINE\n",
    "    for item in item_list:\n",
    "        #item = item - 1                                     ##### REMOVE THIS LINE\n",
    "        if m1_matrix[user,item] == UNOBSERVED:\n",
    "            predicted_rating = user_sim_matrix[user].dot(m1_matrix[:,item]) #TODO This step assumes that UNOBSERVED = 0\n",
    "            if predicted_rating > 5:\n",
    "                predicted_rating = 5\n",
    "            elif predicted_rating < 0:     # TODO: check: can we have predicted_rating<0, if sim>0 & user_ratings>0\n",
    "                predicted_rating = 100\n",
    "            predicted_matrix[user,item] = predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making all predicted ratings < 0 (set as 100 above) as 0\n",
    "\n",
    "# making all zero(0) entries in product_matrix as UNOBSERVED\n",
    "# makes it simpler to identify unpredicted rating in evaluation\n",
    "\n",
    "#dont change the order of steps below\n",
    "predicted_matrix = find_and_replace(data=predicted_matrix, find_value=0, replace_value=UNOBSERVED)\n",
    "predicted_matrix = find_and_replace(data=predicted_matrix, find_value=100, replace_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,  20],\n",
       "       [  0,  20, 100],\n",
       "       [  0,  32,   0],\n",
       "       [ 40,   0,   0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate using RMSE for now, will account for other advanced metrics in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all the available ratings in testset\n",
    "# and for all the predicted rating for those available rating\n",
    "# put them in two separate vectors\n",
    "# get rmse using scikit libraries\n",
    "\n",
    "y_actual  = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "y_predict = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "\n",
    "for testpt in len(test_data_csr):\n",
    "    y_actual  = testpt[2]\n",
    "    y_predict = predicted_matrix[testpt[0], testpt[1]]\n",
    "    if y_predict == UNOBSERVED:       # i.e. we could not generate a rating for this test user item pair\n",
    "        y_predict = AVG_RATING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 62, 180,   3],\n",
       "       [385, 120,   3],\n",
       "       [683, 721,   2],\n",
       "       ..., \n",
       "       [226,  13,   4],\n",
       "       [322, 287,   3],\n",
       "       [478, 107,   4]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNOBSERVED = 0\n",
    "data_matrix = read_csr_data_as_matrix()\n",
    "if data_matrix[data_matrix > 0].size == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('Reading data_matrix: success')\n",
    "else:\n",
    "    print('Reading data_matrix: FAILED')\n",
    "    print( '# of missing ratings: ' + str(N_RATINGS - data_matrix[data_matrix > 0].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
