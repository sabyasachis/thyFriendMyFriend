{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Built and testing on python2\n",
    "import numpy as np\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './ml-100k/u.data' # ml-100k data set has 100k ratings, 943 users and 1682 items\n",
    "#DATA_PATH = './filmtrust/ratings.txt' # filmtrust data set has 35497 ratings, 1508 users and 2071 items\n",
    "\n",
    "# Dataset Parameters\n",
    "DATA_TYPE =  0              # 0: CSR format, 1: 2D matrix format  # TODO: use it\n",
    "DELIMITER = \"\\t\"            # tab separated or comma separated data format\n",
    "FIRST_INDEX = 1\n",
    "N_RATINGS = 100000\n",
    "USERS = 943\n",
    "ITEMS = 1682\n",
    "\n",
    "# Hardcoding values\n",
    "UNOBSERVED = -1\n",
    "GET_PRODUCT_FAIL_RETURN = UNOBSERVED    #TODO: This hardcoding can be removed in future\n",
    "TRAIN_TEST_SPLIT = 0.2  # %age of test ratings wrt train rating ; value in between 0 and 1\n",
    "AVG_RATING = 3          # ratings for which we dont have predicted rating\n",
    "\n",
    "# HyperParameters\n",
    "C1 = 0.2                # probability of edges in training set going to E1\n",
    "C2 = 0.3                # probability of edges in training set going to E2\n",
    "C3 = 1 - C1 - C2\n",
    "RADIUS = 3              # radius of neighborhood, radius = # edges between start and end vertex\n",
    "THRESHOLD = 943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#checks on parameters set in previous cell\n",
    "if C3 <= 0:\n",
    "    print('ERROR: Please set the values of C1 and C2, s.t, C1+C2 < 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for data handling\n",
    "\n",
    "(need to handle for boundary cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function to get data in matrix format for given data in CSR format '''\n",
    "def csr_to_matrix(data_csr, symmetry=False):\n",
    "    \n",
    "    if symmetry:                                 ### TODO: Implement this better\n",
    "        users = max(data_csr[:,0]) + 1\n",
    "        items = max(data_csr[:,1]) + 1\n",
    "        users = items = max(users,items)\n",
    "    else:\n",
    "        users = USERS\n",
    "        items = ITEMS\n",
    "        \n",
    "    data_matrix = np.full((users, items), UNOBSERVED, dtype=int)\n",
    "    for line in data_csr:\n",
    "        data_matrix[line[0]][line[1]] = line[2]\n",
    "        if symmetry:\n",
    "            data_matrix[line[1]][line[0]] = line[2]\n",
    "            \n",
    "    return data_matrix\n",
    "\n",
    "''' Function to get data in CSR format for given data in matrix format '''\n",
    "def matrix_to_csr(data_matrix):             # TODO: Check if it works\n",
    "    data_matrix = np.empty([0,0], dtype=int)\n",
    "    data_csr = np.array([ [i,j,data_matrix[i,j]]\\\n",
    "                          for j in range(len(temp[i]))\\\n",
    "                              for i in range(len(temp))\\\n",
    "                                  if temp[i,j] != UNOBSERVED])\n",
    "    return data_csr\n",
    "\n",
    "''' Function to read data file, given in CSR format\n",
    "    Assuming 1st 3 values of a row as: user_id, item_id, rating '''\n",
    "def read_data_csr(fname, delimiter, dtype=int):\n",
    "    data_csr = np.loadtxt(fname=fname, delimiter=delimiter, dtype=dtype) # Reading data to array\n",
    "    data_csr = data_csr[:, :3]                                           # Extracting 1st 3 columns: 0,1,2\n",
    "    data_csr[:,:2] = data_csr[:,0:2] - FIRST_INDEX                       # Making sure index starts from 0\n",
    "    return data_csr\n",
    "\n",
    "''' Function to read data file, given in matrix format '''\n",
    "# TODO\n",
    "def read_data_matrix():\n",
    "    data_matrix = np.empty([0,0], dtype=int)\n",
    "    return data_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_csr = read_data_csr(fname=DATA_PATH, delimiter=DELIMITER)\n",
    "\n",
    "if data_csr.shape[0] == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('done')\n",
    "else:\n",
    "    print('fail')\n",
    "    #print( '# of missing ratings: ' + str(N_RATINGS - data_csr.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting some constants\n",
    "unique_users = len(np.array(list(set(data_csr[:,0]))))\n",
    "unique_items = len(np.array(list(set(data_csr[:,1]))))\n",
    "\n",
    "USERS = max(data_csr[:,0]) + 1\n",
    "ITEMS = max(data_csr[:,1]) + 1\n",
    "\n",
    "OFFSET = USERS + 10     # TODO: MAKE IT HARDCODING; keep it >= #USERS\n",
    "THRESHOLD =  USERS      # TODO: MAKE IT HARDCODING; distance/similarity threshold for users, who are used in avg weighted estimate computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 943\n",
      "ITEMS: 1682\n",
      "All users and items have at least one rating! Good!\n"
     ]
    }
   ],
   "source": [
    "print('USERS: ' + str(USERS))\n",
    "print('ITEMS: ' + str(ITEMS))\n",
    "\n",
    "if USERS != unique_users:\n",
    "    print('No of users with no ratings: ' + str(USERS - unique_users))\n",
    "if ITEMS != unique_items:\n",
    "    print('No of items with no ratings: ' + str(items - unique_items))\n",
    "if USERS == unique_users and ITEMS == unique_items:\n",
    "    print('All users and items have at least one rating! Good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a train/test split for non negative ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# we use data_csr as it is easy to only shuffle it and accordingly create train and test set\n",
    "np.random.shuffle(data_csr) # inplace shuffle\n",
    "\n",
    "train_sz = int((1 - TRAIN_TEST_SPLIT) * data_csr.shape[0])\n",
    "\n",
    "train_data_csr = data_csr[: train_sz ,:]\n",
    "test_data_csr = data_csr[train_sz : ,:]\n",
    "\n",
    "if train_data_csr.shape[0]+test_data_csr.shape[0] == data_csr.shape[0]:\n",
    "    print('done')\n",
    "else:\n",
    "    print('fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore/Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some data analysis nos and plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Begins from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Sample Splitting\n",
    "\n",
    "Split the matrix edges into 3 parts, M1, M2, M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Again using data_csr is easier to split sample into 3 parts\n",
    "\n",
    "#reshuffling training data can be avoided as it was obtained shuffled already\n",
    "#np.random.shuffle(train_data_csr) # inplace shuffle\n",
    "\n",
    "m1_sz = int(C1 * train_data_csr.shape[0])\n",
    "m2_sz = int(C2 * train_data_csr.shape[0])\n",
    "\n",
    "m1_csr = train_data_csr[              : m1_sz         ,:]\n",
    "m2_csr = train_data_csr[        m1_sz : m1_sz + m2_sz ,:]\n",
    "m3_csr = train_data_csr[m1_sz + m2_sz :               ,:]\n",
    "\n",
    "if m1_csr.shape[0]+m2_csr.shape[0]+m3_csr.shape[0] == train_data_csr.shape[0]:\n",
    "    print('done')\n",
    "else:\n",
    "    print('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m1_csr = np.copy(train_data_csr)                                   ##### REMOVE THIS CELL\n",
    "#m2_csr = np.copy(train_data_csr)\n",
    "#m3_csr = np.copy(train_data_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Expanding the neighborhood\n",
    "\n",
    "use M1 to build neighborhood based on radius *r*\n",
    "\n",
    "Normalize the product of ratings by total no of final movies at the boundary\n",
    "\n",
    "Building BFS tree rooted at each vertex, s/t\n",
    "- each node in a path from user to boundary item is unique\n",
    "- shortest path (#path edges) between user and boundary item\n",
    "- in case of multiple paths (or trees) choose any one path (i.e. any one tree) at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from tqdm import *\n",
    "#FIRST_INDEX = 1\n",
    "#OFFSET = 4 + 6\n",
    "#UNOBSERVED = -1\n",
    "#\n",
    "#m1_csr = read_data_csr(fname='./very_small_graph.txt', delimiter=\"\\t\")   ##### REMOVE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Create adjacency list: dictionary of sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''Step 2.1.1 Convert M1 from csr to matrix format'''\n",
    "# item_ids += OFFSET\n",
    "# so that user_ids != item_ids\n",
    "# and we can create an undirected graph (important to get an edge from item to user)\n",
    "\n",
    "new_m1_csr = np.copy(m1_csr)\n",
    "new_m1_csr[:,1] = new_m1_csr[:,1] + OFFSET\n",
    "new_m1_matrix = csr_to_matrix(new_m1_csr, symmetry=True)\n",
    "\n",
    "print('done')  # TODO : put a check for fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2632/2632 [00:05<00:00, 478.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Step 2.1.2 Create an (unweighted) adjacency list for the graph'''\n",
    "# we still have the 2D matrix for the weights\n",
    "\n",
    "graph = dict()\n",
    "for i in tqdm(range(len(new_m1_matrix))):\n",
    "    temp_set = set()\n",
    "    for j in range(len(new_m1_matrix[i])):\n",
    "        if new_m1_matrix[i,j] > 0:\n",
    "            temp_set.add(j)\n",
    "    graph[i] = temp_set\n",
    "\n",
    "print('done')  # TODO : put a check for fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: For all users: Get path products for all movies\n",
    "\n",
    "(maybe only unrated movies: TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This implementation for this step is inspired from:\n",
    "# http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/\n",
    "\n",
    "import random\n",
    "\n",
    "'''Step 2.2.1: Function gives all possible path from 'start' vertex to 'goal' vertex, inclusive of both '''\n",
    "# radius = # edges between start and end vertex\n",
    "def bfs_paths(graph, start, radius):\n",
    "    queue = [(start, [start])]\n",
    "    while queue:\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        for next in graph[vertex] - set(path):\n",
    "            depth = len(path + [next]) - 1\n",
    "            if depth == radius:\n",
    "                yield path + [next]\n",
    "            else:\n",
    "                queue.append((next, path + [next]))\n",
    "\n",
    "'''Step 2.2.2: Function which returns a dictionary for a given user\n",
    "   where each item represents the key in the dictionary\n",
    "   and it returns a list of lists(paths) from user to item r-hop distance apart'''\n",
    "# help from \n",
    "# https://www.kumari.net/index.php/programming/programmingcat/22-python-making-a-dictionary-of-lists-a-hash-of-arrays\n",
    "def create_item_dict(all_path):\n",
    "    dict_path = dict()\n",
    "    for path in all_path:\n",
    "        r_hop_item = path[-1]\n",
    "        dict_path.setdefault(r_hop_item,[]).append(path) \n",
    "    return dict_path\n",
    "\n",
    "'''Step 2.2.3: Function to get product for r-hop path from user to item\n",
    "   Choose any path at random if #paths > 1'''\n",
    "def get_product(path):\n",
    "    if len(path) < 1:\n",
    "        return GET_PRODUCT_FAIL_RETURN\n",
    "    idx = random.randint(0, len(path)-1)    # in case of multiple paths to same item\n",
    "    p = path[idx]                           # choose any one path at random\n",
    "\n",
    "    product = 1\n",
    "    for i in range(len(p)-1):\n",
    "        product = product * new_m1_matrix[p[i],p[i+1]]\n",
    "    return product\n",
    "\n",
    "'''Function to find and replace some values\n",
    "   for only 1d and 2d numpy arrays'''\n",
    "def find_and_replace(data, find_value, replace_value):\n",
    "    if len(data.shape) == 1:\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == find_value:\n",
    "                data[i] = replace_value\n",
    "    elif len(data.shape) == 2:\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i,j] == find_value:\n",
    "                    data[i,j] = replace_value\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_list = np.array(list(set(data_csr[:,0])))                 ##### UNCOMMENT THIS CELL\n",
    "item_list = np.array(list(set(data_csr[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_list = np.array(list(set(m1_csr[:,0])))                  ##### REMOVE THIS CELL\n",
    "#item_list = np.array(list(set(m1_csr[:,1])))\n",
    "#RADIUS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:55<00:00, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "product_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=int)\n",
    "\n",
    "for user in tqdm(user_list):\n",
    "    all_path = list(bfs_paths(graph, user, radius=RADIUS))  # get a list of all r-hop paths from given user\n",
    "    dict_path = create_item_dict(all_path)                  # create dict of paths from user to individual items\n",
    "    for item in dict_path:\n",
    "        paths = dict_path[item]                             # get the set of user-item paths\n",
    "        product = get_product(paths)                        # get product for a unique user-item path (at random)\n",
    "        product_matrix[user, (item - OFFSET)] = product\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Computing the distances\n",
    "\n",
    "use M2 to get distance (or similarity) between two users based on neighbourhood from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making all unobserved entries in product_matrix as zero\n",
    "# makes it simpler for pearson similarity calculation, probably..\n",
    "\n",
    "product_matrix = find_and_replace(data=product_matrix, find_value=UNOBSERVED, replace_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 166/943 [00:01<00:07, 97.94it/s] /anaconda2/envs/play/lib/python2.7/site-packages/scipy/stats/stats.py:3003: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n",
      "100%|██████████| 943/943 [00:54<00:00, 17.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Currently using simple pearson similarity:\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "user_sim_matrix = np.full((len(user_list), len(user_list)), UNOBSERVED, dtype=float)\n",
    "for user1 in tqdm(user_list):\n",
    "    for user2 in user_list:\n",
    "        if user1 >= user2:\n",
    "            [sim, p_value] = stats.pearsonr(product_matrix[user1], product_matrix[user2])\n",
    "            if np.isnan(sim):                       # TODO: check if this is valid to do?\n",
    "                sim = 0\n",
    "            user_sim_matrix[user1,user2] = user_sim_matrix[user2,user1] = sim\n",
    "            # similarity is between 0 and 1\n",
    "            # therefore, these can be directly used as weights on users' rating for prediction\n",
    "\n",
    "#del product_matrix  #no need in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Averaging datapts to get final estimates\n",
    "\n",
    "use M3 to perform a weighted avg using similarity computed in previous step ; account for threshold n_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m3_matrix = csr_to_matrix(m3_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:50<00:00, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=float)\n",
    "for user in tqdm(user_list):\n",
    "    for item in item_list:\n",
    "        if m3_matrix[user,item] == UNOBSERVED:\n",
    "            # we basically do a dot product but avoid taking UNOBSERVED user similarities or item ratings\n",
    "            predicted_rating = user_sim_matrix[user, m3_matrix[:,item] != UNOBSERVED      ]\\\n",
    "                                .dot(m3_matrix[      m3_matrix[:,item] != UNOBSERVED, item])\n",
    "                \n",
    "            #if np.isnan(predicted_rating):\n",
    "            #    print(user,item)\n",
    "            \n",
    "            if predicted_rating > 5:\n",
    "                predicted_rating = 5\n",
    "            elif predicted_rating < 1:\n",
    "                predicted_rating = 1\n",
    "            predicted_matrix[user,item] = predicted_rating\n",
    "\n",
    "#del user_sim_matrix\n",
    "print('done')  # TODO : put a check for fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate using RMSE for now, will account for other advanced metrics in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 239108.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for all the available ratings in testset\n",
    "# and for all the predicted rating for those available rating\n",
    "# put them in two separate vectors\n",
    "# get rmse using scikit libraries\n",
    "\n",
    "y_actual  = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "y_predict = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "\n",
    "for i in tqdm(range(len(test_data_csr))):\n",
    "    testpt = test_data_csr[i]\n",
    "    y_actual[i]  = testpt[2]\n",
    "    y_predict[i] = predicted_matrix[testpt[0], testpt[1]]\n",
    "    if y_predict[i] == UNOBSERVED:       # i.e. we could not generate a rating for this test user item pair\n",
    "        y_predict[i] = AVG_RATING\n",
    "\n",
    "#del predicted_matrix\n",
    "print('done')  # TODO : put a check for fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8278700378337203"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_actual, y_predict))\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Rough work: functions to be used for debugging, or no longer neede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata_matrix = read_csr_data_as_matrix()\\nif data_matrix[data_matrix > 0].size == N_RATINGS:  # gives total no of ratings read; useful for verification\\n    print('Reading data_matrix: success')\\nelse:\\n    print('Reading data_matrix: FAILED')\\n    print( '# of missing ratings: ' + str(N_RATINGS - data_matrix[data_matrix > 0].size))\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function to read CSR data file in matrix format\n",
    "import csv\n",
    "def read_csr_data_as_matrix(users=USERS, items=ITEMS, symmetry=False):\n",
    "    if symmetry:\n",
    "        users = items = max(users, items)\n",
    "    data_matrix = np.full((users, items), UNOBSERVED, dtype=int)\n",
    "    with open(DATA_PATH) as tsv:\n",
    "        for line in csv.reader(tsv, delimiter=\"\\t\"):    # line is a list containing all tab separated items\n",
    "            line = [int(i) for i in line]               # convert the list of str to list of int\n",
    "            data_matrix[line[0] - FIRST_INDEX][line[1] - FIRST_INDEX] = line[2]\n",
    "    return data_matrix\n",
    "'''\n",
    "data_matrix = read_csr_data_as_matrix()\n",
    "if data_matrix[data_matrix > 0].size == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('Reading data_matrix: success')\n",
    "else:\n",
    "    print('Reading data_matrix: FAILED')\n",
    "    print( '# of missing ratings: ' + str(N_RATINGS - data_matrix[data_matrix > 0].size))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function gives all possible path from 'start' vertex to 'goal' vertex, inclusive of both\n",
    "def old_bfs_paths(graph, start, goal):\n",
    "    queue = [(start, [start])]\n",
    "    while queue:\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        for next in graph[vertex] - set(path):\n",
    "            if next == goal:\n",
    "                yield path + [next]\n",
    "            else:\n",
    "                queue.append((next, path + [next]))\n",
    "\n",
    "\n",
    "# Function to get paths from user to item at r-hop distance\n",
    "# assuming self rated movies are at radius = 0 and direct friends' movie at radius = 2\n",
    "def get_r_hop_path(path, radius=RADIUS):\n",
    "    exact_path = []\n",
    "    for p in path:\n",
    "        if ((len(p)/2) - 1) == radius:\n",
    "            exact_path.append(p)\n",
    "    return exact_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 2.2.1: Function gives all possible path from 'start' vertex to 'goal' vertex, inclusive of both\n",
    "def new_bfs_paths(graph, start, radius):\n",
    "    print('Start: ' + str(start))\n",
    "    print(\" \")\n",
    "    queue = [(start, [start])]\n",
    "    while queue:\n",
    "        print(\"Begin: \" + str(queue))\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        print('Mid: ' + str(queue))\n",
    "        print('Vertex: ' + str(vertex) + ' ; path: ' + str(path))\n",
    "        for next in graph[vertex] - set(path):\n",
    "            print('Next: ' + str(next))\n",
    "            depth = len(path + [next]) - 1\n",
    "            if depth == radius:\n",
    "                print('Max depth reached: ' + str(depth))\n",
    "                yield path + [next]\n",
    "            #if next == goal:\n",
    "            #    print('Goal reached, final path: ' + str(path + [next]))\n",
    "            else:\n",
    "                print('Appended: ' + str(path + [next]))\n",
    "                queue.append((next, path + [next]))\n",
    "            print('Depth: ' + str(depth))\n",
    "        print(\" \")\n",
    "\n",
    "\n",
    "#path = list(new_bfs_paths(graph, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
