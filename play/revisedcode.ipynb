{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of NIPS 2017 paper, titled [Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation](https://papers.nips.cc/paper/7057-thy-friend-is-my-friend-iterative-collaborative-filtering-for-sparse-matrix-estimation.pdf) for [Global NIPS Paper implementation Challenge](https://www.nurture.ai/nips-challenge). The authors have also created a short [video](https://www.youtube.com/watch?v=qxfDK44YuQE) for it.<br>\n",
    "For any questions related to this notebook, feel free to contact me at ssahoo.infinity@gmail.com ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this Notebook:\n",
    "Following are some important points/guidelines/assumptions to keep in mind while navigating through this notebook:\n",
    "- This notebook consists of sections numbered by Roman numerals\n",
    "- Brief description about all sections:\n",
    "  - I: Model preparation: Changes to be made to rating matrix(dataset) before we can apply the algorithms\n",
    "  - II: Algorithm Details: All the algorithms described in the paper are implemented here.\n",
    "  - III: Other important functions:\n",
    "    - Data Handling: for data manipulation\n",
    "    - Substitutes: methods which can be used in place of (algorithm) methods in paper\n",
    "    - Evaluation: for evaulation of recommender system\n",
    "  - IV: Test script/Experiments: testing using a dataset\n",
    "- If a function/variable has suffix as \"_csr\", it refers to CSR (Compressed Sparse Row) data. Else, if it has a suffix as \"_matrix\", it refers to 2D (numpy) matrix data.\n",
    "- The dataset ratings are assumed to be integers; to be modified in future\n",
    "- data_csr ensures that user_id and item_id start from 0 by taking in FIRST_INDEX as a global variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required modules required for complete notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Built and tested on python2\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Model Notations\n",
    "Following are the symbols/notations used in the paper. The variables/notations used in this notebook have been discussed in Experiments section.\n",
    "\n",
    "$u$ = user <br>\n",
    "$i$ = item <br>\n",
    "$M$ = symmetric rating matrix of size $n \\times n$ (usually the dataset) <br>\n",
    "$E$ = set of $(u,i)$ where each user $u$ has rated an item $i$ also seen in the matrix $M$ (intuitively $E$ is edge set(matrix) between user and items. <br>\n",
    "$p$ = sparsity of $M$ i.e. (= #observed ratings in $M$ / total # ratings in $M$)<br>\n",
    "$r$ = radius, distance (in no of edges) between user $u$ and item $i$ at neighborhood boundary (look in step 2) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I: Model preparation\n",
    "We first look at function which converts our asymmetric rating matrix to a symmetric matrix and another function that normalizes the ratings between [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to normalize all ratings of a CSR (compressed sparse row) matrix'''\n",
    "def normalize_ratings_csr(data_csr):\n",
    "    \n",
    "    return data_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Function to convert asymmetrix matrix to symmetrix matrix'''\n",
    "def convert_to_symmetric_matrix(data_matrix):\n",
    "    #check if matrix is already symmetric?\n",
    "    \n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II: Algorithm Details\n",
    "As per paper: *We present and discuss details of each step of the algorithm, which primarily involves computing pairwise distances (or similarities) between vertices.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Sample Splitting\n",
    "Partition the rating matrix into three different parts. Following are the exerpts from paper:\n",
    "- *Each edge in $E$ is independently placed into $E_1, E_2,$ or $E_3$, with probabilities $c_1, c_2,$ and $1 - c_1 - c_2$ respectively. Matrices $M_1, M_2$, and $M_3$ contain information from the subset of the data in $M$ associated to $E_1, E_2$, and $E_3$ respectively.*\n",
    "- *$M_1$ is used to define local neighborhoods of each vertex (in step 2), $M_2$ is used to compute similarities of these neighborhoods (in step 3), and $M_3$ is used to average over datapoints for the final estimate (in step 4)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_splitting_csr(data_csr, c1=0.3, c2=0.3, shuffle=False):\n",
    "    if (shuffle):\n",
    "        np.random.shuffle(data_csr) # inplace shuffle\n",
    "\n",
    "    m1_sz = int(c1 * data_csr.shape[0])\n",
    "    m2_sz = int(c2 * data_csr.shape[0])\n",
    "\n",
    "    m1_csr = data_csr[              : m1_sz         ,:]\n",
    "    m2_csr = data_csr[        m1_sz : m1_sz + m2_sz ,:]\n",
    "    m3_csr = data_csr[m1_sz + m2_sz :               ,:]\n",
    "\n",
    "    if m1_csr.shape[0]+m2_csr.shape[0]+m3_csr.shape[0] == data_csr.shape[0]:\n",
    "        print('Sample splitting: done')\n",
    "    else:\n",
    "        print('Sample splitting: FAILED')\n",
    "        \n",
    "    return [m1_csr, m2_csr, m3_csr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Expanding the Neighborhood\n",
    "We do the following in this step:\n",
    "- radius $r$ to be tuned using cross validation. We can use its default value as $r = \\frac{6\\ln(1/p)}{8\\ln(c_1pn)}$ as per paper.\n",
    "- use matrix $M_1$ to build neighborhood based on radius $r$\n",
    "- Build BFS tree rooted at each vertex to get product of the path from user to item, such that\n",
    "  - each vertex (user or item) in a path from user to boundary item is unique\n",
    "  - the path chosen is the shortest path (#path edges) between the user and the boundary item\n",
    "  - in case of multiple paths (or trees), choose any one path (i.e. any one tree) at random\n",
    "- Normalize the product of ratings by total no of final items at the boundary\n",
    "\n",
    "$N_{u,r}$ obtained is a vector for user $u$ for $r$-hop, where each element is product of path from user to item or zero. $\\tilde{N_{u,r}}$ is normalized vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function get matrix from csr such that no two item_ids and user_ids are same'''\n",
    "#useful for finding paths in graph\n",
    "def csr_to_matrix_with_offset(data_csr): #TODO: put it in data handling\n",
    "    # Convert M1 from csr to matrix format\n",
    "    ## item_ids += OFFSET\n",
    "    ## so that user_ids != item_ids\n",
    "    ## and we can create an undirected graph (important to get an edge from item to user)\n",
    "    new_data_csr = np.copy(data_csr)\n",
    "    new_data_csr[:,1] = new_data_csr[:,1] + OFFSET\n",
    "    new_data_matrix = csr_to_matrix(new_data_csr, symmetry=True)\n",
    "    return new_data_matrix\n",
    "\n",
    "'''Function to create a graph as adjacency list: a dictionary of sets'''\n",
    "def create_dict_graph_from_csr(data_csr):\n",
    "    new_data_matrix = csr_to_matrix_with_offset(data_csr)\n",
    "    # Create an (unweighted) adjacency list for the graph\n",
    "    ## we still have the 2D matrix for the weights\n",
    "    graph = dict()\n",
    "    print('Creating graph as dictionary:')\n",
    "    sys.stdout.flush()\n",
    "    for i in tqdm(range(len(new_data_matrix))):\n",
    "        temp_set = set()\n",
    "        for j in range(len(new_data_matrix[i])):\n",
    "            if new_data_matrix[i,j] > 0:\n",
    "                temp_set.add(j)\n",
    "        graph[i] = temp_set\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions useful for getting products along paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''Function gives all possible path from 'start' vertex to 'goal' vertex, inclusive of both '''\n",
    "# help from:\n",
    "# http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/\n",
    "# radius = # edges between start and end vertex\n",
    "def bfs_paths(graph, start, radius):\n",
    "    queue = [(start, [start])]\n",
    "    while queue:\n",
    "        (vertex, path) = queue.pop(0)\n",
    "        for next in graph[vertex] - set(path):\n",
    "            depth = len(path + [next]) - 1\n",
    "            if depth == radius:\n",
    "                yield path + [next]\n",
    "            else:\n",
    "                queue.append((next, path + [next]))\n",
    "\n",
    "'''Function which returns a dictionary for a given user\n",
    "   where each item represents the key in the dictionary\n",
    "   and it returns a list of lists(paths) from user to item r-hop distance apart'''\n",
    "def create_item_dict(all_path):\n",
    "    dict_path = dict()\n",
    "    for path in all_path:\n",
    "        r_hop_item = path[-1]\n",
    "        dict_path.setdefault(r_hop_item,[]).append(path) \n",
    "    return dict_path\n",
    "\n",
    "'''Function to get product for r-hop path from user to item\n",
    "   Choose any path at random if #paths > 1'''\n",
    "def get_product(new_data_matrix, path):\n",
    "    if len(path) < 1:\n",
    "        return GET_PRODUCT_FAIL_RETURN\n",
    "    idx = random.randint(0, len(path)-1)    # in case of multiple paths to same item\n",
    "    p = path[idx]                           # choose any one path at random\n",
    "\n",
    "    product = 1\n",
    "    for i in range(len(p)-1):\n",
    "        product = product * new_data_matrix[p[i],p[i+1]]\n",
    "    return product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "'''Function to generate product matrix from user to items\n",
    "   (items which are at r-hop boundary from user)'''\n",
    "# if radius passed is less than 1 or not passed, this function evaluates the default radius as per paper\n",
    "def generate_product_matrix(data_csr, m1_csr, c1, radius=0):\n",
    "    # TODO: fix it's default radius case for symmetric matrix case\n",
    "    user_list = np.array(list(set(data_csr[:,0])))\n",
    "    item_list = np.array(list(set(data_csr[:,1])))\n",
    "    n_ratings = len(data_csr)\n",
    "    n_users = len(user_list)\n",
    "    n_items = len(item_list)\n",
    "    sparsity = float(    n_ratings) /  (n_users * n_items)\n",
    "    \n",
    "    #TODO: Find a fix for this; why is it coming less than 1?\n",
    "    #print((float(6) * math.log( 1 / float(sparsity))) / (8 * math.log( c1 * sparsity * min(n_users,n_items))))\n",
    "    if radius < 1:\n",
    "        radius = (float(6) * math.log( 1 / float(sparsity))) / (8 * math.log( c1 * sparsity * min(n_users,n_items)))\n",
    "\n",
    "    # First create the graph\n",
    "    new_data_matrix = csr_to_matrix_with_offset(m1_csr)\n",
    "    graph = create_dict_graph_from_csr(m1_csr)\n",
    "    \n",
    "    # Get bfs-path products from the graph\n",
    "    product_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=int)\n",
    "    print('Generating product matrix:')\n",
    "    sys.stdout.flush()\n",
    "    for user in tqdm(user_list):\n",
    "        all_path = list(bfs_paths(graph, user, radius))       # 1. get a list of all r-hop paths from given user\n",
    "        dict_path = create_item_dict(all_path)                # 2. create dict of paths from user to individual items\n",
    "        for item in dict_path:\n",
    "            paths = dict_path[item]                           # 3. get the set of user-item paths\n",
    "            product = get_product(new_data_matrix, paths)       # 4. get product for a unique user-item path (at random)\n",
    "            product_matrix[user, (item - OFFSET)] = product   # 5. store the product in the matrix\n",
    "    return product_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Computing the distances\n",
    "Distance computation between two users (using matrix $M_2$) using the following formula (only $dist_1$ implemented for now):\n",
    "\n",
    "$$ dist(u,v) = \\left(\\frac{1 - c_1p}{c_2p}\\right) (\\tilde{N_{u,r}} - \\tilde{N_{v,r}})^T M_2 (\\tilde{N_{u,r+1}} - \\tilde{N_{v,r+1}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def generate_user_sim_matrix(data_csr, m1_csr, product_matrix):\n",
    "    # making all unobserved entries in product_matrix as zero\n",
    "    # makes it simpler for pearson similarity calculation, probably..\n",
    "    product_matrix = find_and_replace(data=product_matrix, find_value=UNOBSERVED, replace_value=0)\n",
    "    user_list = np.array(list(set(data_csr[:,0])))\n",
    "    item_list = np.array(list(set(data_csr[:,1])))\n",
    "\n",
    "    # Currently using simple pearson similarity:\n",
    "    user_sim_matrix = np.full((len(user_list), len(user_list)), UNOBSERVED, dtype=float)\n",
    "    print('Generating user sim matrix (pearson similarity):')\n",
    "    sys.stdout.flush()\n",
    "    for user1 in tqdm(user_list):\n",
    "        for user2 in user_list:\n",
    "            if user1 >= user2:\n",
    "                [sim, p_value] = stats.pearsonr(product_matrix[user1], product_matrix[user2])\n",
    "                if np.isnan(sim):                       # TODO: check if this is valid to do?\n",
    "                    sim = 0\n",
    "                user_sim_matrix[user1,user2] = user_sim_matrix[user2,user1] = sim\n",
    "                # similarity is between -1 and 1\n",
    "                # therefore, these can be directly used as weights on users' rating for prediction\n",
    "    return user_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Averaging datapoints to produce final estimate\n",
    "Average over nearby data points based on the distance(similarity) threshold $n_n$ (using matrix $M_3$). $n_n$ to be tuned using cross validation. Mathematically (from paper):\n",
    "\n",
    "$$ \\hat{F_{u,v}} = \\frac{1}{\\mid E_{uv1} \\mid} \\sum_{(a,b) \\in E_{uv1}} M_3(a,b) $$\n",
    "*where $E_{uv1}$ denotes the set of undirected edges $(a, b)$ such that $(a, b) \\in E_3$ and both $dist(u, a)$ and $dist(v, b)$ are less than $n_n$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generated_unweighted_averaged_prediction_matrix(data_csr, m3_csr, user_sim_matrix, bounded=True):\n",
    "    predicted_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=float)\n",
    "    #actually used the matrix below for this fn\n",
    "    return predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generated_weighted_averaged_prediction_matrix(data_csr, m3_csr, user_sim_matrix, bounded=True):\n",
    "    m3_matrix = csr_to_matrix(m3_csr)\n",
    "    user_list = np.array(list(set(data_csr[:,0])))\n",
    "    item_list = np.array(list(set(data_csr[:,1])))\n",
    "\n",
    "    predicted_matrix = np.full((len(user_list), len(item_list)), UNOBSERVED, dtype=float)\n",
    "    print('Generating prediction matrix:')\n",
    "    sys.stdout.flush()\n",
    "    for user in tqdm(user_list):\n",
    "        for item in item_list:\n",
    "            if m3_matrix[user,item] == UNOBSERVED:\n",
    "                # we basically do a dot product but avoid taking UNOBSERVED user similarities or item ratings\n",
    "                predicted_rating = user_sim_matrix[user, m3_matrix[:,item] != UNOBSERVED      ]\\\n",
    "                                    .dot(m3_matrix[      m3_matrix[:,item] != UNOBSERVED, item])\n",
    "                if bounded:\n",
    "                    if predicted_rating > 5:\n",
    "                        predicted_rating = 5\n",
    "                    elif predicted_rating < 1:\n",
    "                        predicted_rating = 1\n",
    "                predicted_matrix[user,item] = predicted_rating\n",
    "    return predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III: Other important functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function to get data in matrix format for given data in CSR format '''\n",
    "def csr_to_matrix(data_csr, symmetry=False):\n",
    "    \n",
    "    if symmetry:                                 ### TODO: Implement this better\n",
    "        users = max(data_csr[:,0]) + 1\n",
    "        items = max(data_csr[:,1]) + 1\n",
    "        users = items = max(users,items)\n",
    "    else:\n",
    "        users = USERS\n",
    "        items = ITEMS\n",
    "        \n",
    "    data_matrix = np.full((users, items), UNOBSERVED, dtype=int)\n",
    "    for line in data_csr:\n",
    "        data_matrix[line[0]][line[1]] = line[2]\n",
    "        if symmetry:\n",
    "            data_matrix[line[1]][line[0]] = line[2]\n",
    "            \n",
    "    return data_matrix\n",
    "\n",
    "''' Function to get data in CSR format for given data in matrix format '''\n",
    "def matrix_to_csr(data_matrix):             # TODO: Check if it works\n",
    "    data_matrix = np.empty([0,0], dtype=int)\n",
    "    data_csr = np.array([ [i,j,data_matrix[i,j]]\\\n",
    "                          for j in range(len(temp[i]))\\\n",
    "                              for i in range(len(temp))\\\n",
    "                                  if temp[i,j] != UNOBSERVED])\n",
    "    return data_csr\n",
    "\n",
    "''' Function to read data file, given in CSR format\n",
    "    Assuming 1st 3 values of a row as: user_id, item_id, rating '''\n",
    "def read_data_csr(fname, delimiter, dtype=int):\n",
    "    data_csr = np.loadtxt(fname=fname, delimiter=delimiter, dtype=dtype) # Reading data to array\n",
    "    data_csr = data_csr[:, :3]                                           # Extracting 1st 3 columns: 0,1,2\n",
    "    data_csr[:,:2] = data_csr[:,0:2] - FIRST_INDEX                       # Making sure index starts from 0\n",
    "    return data_csr\n",
    "\n",
    "'''Function to find and replace some values\n",
    "   for only 1d and 2d numpy arrays'''\n",
    "def find_and_replace(data, find_value, replace_value):\n",
    "    if len(data.shape) == 1:\n",
    "        for i in range(len(data)):\n",
    "            if data[i] == find_value:\n",
    "                data[i] = replace_value\n",
    "    elif len(data.shape) == 2:\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i,j] == find_value:\n",
    "                    data[i,j] = replace_value\n",
    "    return data\n",
    "\n",
    "''' Function to check dataset'''\n",
    "#TODO: make it set some hardcodings\n",
    "def check_dataset_csr(data_csr):\n",
    "    unique_users = len(np.array(list(set(data_csr[:,0]))))\n",
    "    unique_items = len(np.array(list(set(data_csr[:,1]))))\n",
    "    n_users = max(data_csr[:,0]) + 1\n",
    "    n_items = max(data_csr[:,1]) + 1\n",
    "    \n",
    "    print('USERS: ' + str(n_users))\n",
    "    print('ITEMS: ' + str(n_items))\n",
    "    \n",
    "    # checking unrated users/items TODO: this is not possible if given data is in CSR\n",
    "    if n_users != unique_users:\n",
    "        print('No of users with no ratings: ' + str(n_users - unique_users))\n",
    "    if n_items != unique_items:\n",
    "        print('No of items with no ratings: ' + str(n_items - unique_items))\n",
    "    if n_users == unique_users and n_items == unique_items:\n",
    "        print('All users and items have at least one rating! Good!')\n",
    "    \n",
    "    #checking sparsity\n",
    "    #TODO: here we assume given dataset is rectangular; UNDO this assumption\n",
    "    n_ratings = data_csr.shape[0]\n",
    "    sparsity_asymm = float(    n_ratings) /  (n_users * n_items)     # sparsity of given rating matrix\n",
    "    sparsity_symm =  float(2 * n_ratings) / ((n_users + n_items)**2) # sparsity of large symmetricized matrix\n",
    "    print('Sparsity of given matrix p: ' + str(sparsity_asymm))\n",
    "    print('Sparsity of large symmetricized matrix p: ' + str(sparsity_symm))\n",
    "    if sparsity_asymm <= (float(1) / max(n_users, n_items)):\n",
    "        print('WARNING: For given rectangular asymmetric matrix:')\n",
    "        print('       : p is not polynomially larger than 1/n.')\n",
    "        print('       : Using dist1 as distance computation may not gurantee that')\n",
    "        print('       : expected square error converges to zero using this paper\\'s algorithm.')\n",
    "    else:\n",
    "        print('Asymm matrix: p is polynomially larger than 1/n, all guarantees applicable')\n",
    "    if sparsity_symm <= (float(1) / ((n_users + n_items)**2)):\n",
    "        print('WARNING: For generated large symmetric matrix:')\n",
    "        print('       : p is not polynomially larger than 1/n.')\n",
    "        print('       : Using dist1 as distance computation may not gurantee that')\n",
    "        print('       : expected square error converges to zero using this paper\\'s algorithm.')\n",
    "    else:\n",
    "        print('Sym matrix : p is polynomially larger than 1/n, all guarantees applicable')\n",
    "\n",
    "'''Function to generate training and testing data split from given CSR data'''\n",
    "def generate_train_test_split_csr(data_csr, split, shuffle=True):\n",
    "    # we use data_csr as it is easy to only shuffle it and accordingly create train and test set\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data_csr) # inplace shuffle\n",
    "\n",
    "    train_sz = int((1 - split) * data_csr.shape[0])\n",
    "\n",
    "    train_data_csr = data_csr[: train_sz ,:]\n",
    "    test_data_csr = data_csr[train_sz : ,:]\n",
    "\n",
    "    if train_data_csr.shape[0]+test_data_csr.shape[0] == data_csr.shape[0]:\n",
    "        print('Generating train test split: done')\n",
    "    else:\n",
    "        print('Generating train test split: FAILED')\n",
    "    return [train_data_csr, test_data_csr]\n",
    "\n",
    "''' Function to read data file, given in matrix format '''\n",
    "# TODO\n",
    "def read_data_matrix():\n",
    "    data_matrix = np.empty([0,0], dtype=int)\n",
    "    return data_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute functions\n",
    "Functions which can also be used instead of algorithm specific implementations for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We evaluate our recommendation algorithm using RMSE (root mean square error). <br>\n",
    "According to paper, if sparsity $p$ is polynomially larger than $n^{-1}$, i.e. if $p = n^{-1 + \\epsilon}$ for $\\epsilon > 0$, then we can safely use $dist_1$ distance computation formula and MSE is bounded by $O((pn)^{-1/5})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def generate_true_and_test_labels(test_data_csr, predicted_matrix):\n",
    "    # for all the available ratings in testset\n",
    "    # and for all the predicted rating for those available rating\n",
    "    # put them in two separate vectors\n",
    "    y_actual  = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "    y_predict = np.full((len(test_data_csr)), UNOBSERVED, dtype=float)\n",
    "\n",
    "    print('Generating true and test label:')\n",
    "    sys.stdout.flush()\n",
    "    for i in tqdm(range(len(test_data_csr))):\n",
    "        testpt = test_data_csr[i]\n",
    "        y_actual[i]  = testpt[2]\n",
    "        y_predict[i] = predicted_matrix[testpt[0], testpt[1]]\n",
    "        if y_predict[i] == UNOBSERVED:       # i.e. we could not generate a rating for this test user item pair\n",
    "            y_predict[i] = AVG_RATING\n",
    "    return [y_actual, y_predict]\n",
    "\n",
    "\n",
    "def get_mse(y_actual, y_predict):\n",
    "    mse = mean_squared_error(y_actual, y_predict)\n",
    "    return mse\n",
    "    \n",
    "def get_rmse(y_actual, y_predict):\n",
    "    rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
    "    return rmse\n",
    "    \n",
    "def get_avg_err(y_actual, y_predict):\n",
    "    avg_err = sum(abs(y_actual - y_predict)) / len(y_actual)\n",
    "    return avg_err\n",
    "\n",
    "def check_mse(data_csr, y_actual, y_predict):\n",
    "    #TODO: implement it properly accounting for larger symmetric matrix\n",
    "    n_users = max(data_csr[:,0]) + 1\n",
    "    n_items = max(data_csr[:,1]) + 1\n",
    "    n_ratings = data_csr.shape[0]\n",
    "    sparsity = float(    n_ratings) /  (n_users * n_items)\n",
    "    mse_upper_bound = (sparsity * max(n_users, n_items)) ** (-1 / float(5))\n",
    "    mse = get_mse(y_actual, y_predict)\n",
    "    if mse < mse_upper_bound:\n",
    "        print('As per the discussion in the paper, MSE is bounded by O((pn)**(-1/5))')\n",
    "    else:\n",
    "        print('ERROR: Contrary to the discusssion in the paper, MSE is NOT bounded by O((pn)**(-1/5))')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV: Test Script / Experiment\n",
    "The following jupyter notebook cells make calls to above cells to run experiments on a recommendation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dataset Parameters'''\n",
    "DATA_PATH = './ml-100k/u.data' # ml-100k data set has 100k ratings, 943 users and 1682 items\n",
    "#DATA_TYPE =  0              # 0: CSR format, 1: 2D matrix format  # TODO: use it\n",
    "DELIMITER = \"\\t\"             # tab separated or comma separated data format\n",
    "FIRST_INDEX = 1\n",
    "N_RATINGS = 100000\n",
    "USERS = 943\n",
    "ITEMS = 1682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Hyperparameters'''\n",
    "C1 = 0.2                # probability of edges in training set going to E1\n",
    "C2 = 0.3                # probability of edges in training set going to E2\n",
    "C3 = 1 - C1 - C2        # probability of edges in training set going to E3\n",
    "RADIUS = 3              # radius of neighborhood, radius = # edges between start and end vertex, keep it -1 to use default value given in paper\n",
    "THRESHOLD = 943\n",
    "\n",
    "#checks on parameters\n",
    "if C3 <= 0:\n",
    "    print('ERROR: Please set the values of C1 and C2, s.t, C1+C2 < 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Hardcoding values'''\n",
    "OFFSET = USERS + 10                     # offset so that user_id and item_id are different in graph; keep it >= #USERS\n",
    "UNOBSERVED = -1\n",
    "GET_PRODUCT_FAIL_RETURN = UNOBSERVED    #TODO: This hardcoding can be removed in future\n",
    "TRAIN_TEST_SPLIT = 0.2                  # %age of test ratings wrt train rating ; value in between 0 and 1\n",
    "AVG_RATING = 3                          # ratings for which we dont have predicted rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset: done\n"
     ]
    }
   ],
   "source": [
    "data_csr = read_data_csr(fname=DATA_PATH, delimiter=DELIMITER)\n",
    "\n",
    "if data_csr.shape[0] == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('Reading dataset: done')\n",
    "else:\n",
    "    print('Reading dataset: FAILED')\n",
    "    #print( '# of missing ratings: ' + str(N_RATINGS - data_csr.shape[0]))  #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 943\n",
      "ITEMS: 1682\n",
      "All users and items have at least one rating! Good!\n",
      "Sparsity of given matrix p: 0.0630466936422\n",
      "Sparsity of large symmetricized matrix p: 0.0290249433107\n",
      "Asymm matrix: p is polynomially larger than 1/n, all guarantees applicable\n",
      "Sym matrix : p is polynomially larger than 1/n, all guarantees applicable\n"
     ]
    }
   ],
   "source": [
    "check_dataset_csr(data_csr=data_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO : normalize the ratings and symmtericize the given matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train test split: done\n"
     ]
    }
   ],
   "source": [
    "[train_data_csr, test_data_csr] = generate_train_test_split_csr(data_csr=data_csr, split=TRAIN_TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using THE algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Sample splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splitting: done\n"
     ]
    }
   ],
   "source": [
    "[m1_csr, m2_csr, m3_csr] = sample_splitting_csr(data_csr=data_csr, c1=C1, c2=C2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Expanding the Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph as dictionary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2635/2635 [00:04<00:00, 572.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating product matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 943/943 [01:40<00:00,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "product_matrix = generate_product_matrix(data_csr, m1_csr, c1=C1, radius=RADIUS)\n",
    "#TODO: check why generating product matrix is taking about a minute longer w.r.t. rawcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Computing the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating user sim matrix (pearson similarity):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:52<00:00, 17.83it/s] \n"
     ]
    }
   ],
   "source": [
    "user_sim_matrix = generate_user_sim_matrix(data_csr, m1_csr, product_matrix)\n",
    "# del product_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Averaging datapoints to produce final estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [00:49<00:00, 19.00it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_matrix = generated_weighted_averaged_prediction_matrix(data_csr, m3_csr, user_sim_matrix, bounded=True)\n",
    "# del user_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating true and test label:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 219212.01it/s]\n"
     ]
    }
   ],
   "source": [
    "[y_actual, y_predict] = generate_true_and_test_labels(test_data_csr, predicted_matrix)\n",
    "# del predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2436840434772813"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rmse(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99914999999999998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_err(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Contrary to the discusssion in the paper, MSE is NOT bounded by O((pn)**(-1/5))\n"
     ]
    }
   ],
   "source": [
    "check_mse(data_csr, y_actual, y_predict) # TODO: this might be because the matrix considered here is not symmetric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
