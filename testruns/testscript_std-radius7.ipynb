{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test script\n",
    "This is a standard test script which can be used to simply give a dataset as input(make sure the dataset is in CSR format with 1st 3 entries as [user_id, col_id, ratings, ... ] and ratings are in between 1 - 5. Else, you will have to make changes to original jupyter notebook) and get performance evaluation of THE algorithm. <br>\n",
    "There also maybe some substitute functions for your use other than those mentioned in this testscript. Look into the original jupyter notebook for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the main Jupyter notebook which has all the functions defined. Make sure the path is correct in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(12, 21, 43, 315943)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().time()     # (hour, min, sec, microsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if this way of importing another jupyter notebook fails for you\n",
    "# then you can use any one of the many methods described here:\n",
    "# https://stackoverflow.com/questions/20186344/ipynb-import-another-ipynb-file\n",
    "%run '../src/finalcode.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(12, 21, 44, 946750)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().time()     # (hour, min, sec, microsec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dataset Parameters'''\n",
    "################################################################################################################\n",
    "DATA_PATH = '../data/ml-100k/u.data' # ml-100k data set has 100k ratings, 943 users and 1682 items\n",
    "DELIMITER = \"\\t\"               # tab separated or comma separated data format\n",
    "N_RATINGS = 100000\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These parameters will be detected automatically from dataset\n",
    "# -1 is for the default value\n",
    "FIRST_INDEX = -1\n",
    "USERS = -1\n",
    "ITEMS = -1\n",
    "SPARSITY = -1                  # 'p' in the equations\n",
    "UNOBSERVED = 0                 # default value in matrix for unobserved ratings; prefer to keep it 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To reduce size of csr for testing purpose\n",
    "# WARNING: ONLY TO BE USED FOR TESTING\n",
    "# (for real run, put SIZE_REDUCTION = False)\n",
    "SIZE_REDUCTION = False\n",
    "#USER_LIMIT = 50\n",
    "#ITEM_LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Hyperparameters'''\n",
    "# All the hyperparameters have default values\n",
    "#To use them, set the parameters as -1\n",
    "################################################################################################################\n",
    "TRAIN_TEST_SPLIT = -1                   # %age of test ratings wrt train rating ; value in between 0 and 1\n",
    "C1 = -1                                 # probability of edges in training set going to E1\n",
    "C2 = -1                                 # probability of edges in training set going to E2\n",
    "RADIUS = 7                              # radius of neighborhood, radius = # edges between start and end vertex\n",
    "UNPRED_RATING = 3                       # rating (normalized) for which we dont have predicted rating between 1 - 5\n",
    "THRESHOLD = 0.01                        # distance similarity threshold used for rating prediction\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 = 0.33 (default)\n",
      "c2 = 0.33 (default)\n",
      "c3 = 0.34 (default)\n",
      "Radius = 7\n",
      "Rating set for unpredicted ratings = 3\n",
      "TRAIN_TEST_SPLIT = 0.2 (default)\n"
     ]
    }
   ],
   "source": [
    "# checks on hyper parameters    \n",
    "if isinstance(C1, float) and isinstance(C2, float) and (C1 > 0) and (C2 > 0) and 1 - C1 - C2 > 0:\n",
    "    print('c1 = {}'.format(C1))\n",
    "    print('c2 = {}'.format(C2))\n",
    "    print('c3 = {}'.format(1-C1-C2))\n",
    "elif (C1 == -1) and (C2 == -1):\n",
    "    C1 = C2 = 0.33\n",
    "    print('c1 = {} (default)'.format(C1))\n",
    "    print('c2 = {} (default)'.format(C2))\n",
    "    print('c3 = {} (default)'.format(1-C1-C2))\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for C1 and C2')\n",
    "    \n",
    "if isinstance(RADIUS, int) and RADIUS > 0:\n",
    "    print('Radius = {}'.format(RADIUS))\n",
    "elif RADIUS == -1:\n",
    "    print('Radius = default value as per paper')\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for Radius')\n",
    "\n",
    "if UNPRED_RATING >= 1 and UNPRED_RATING <= 5:\n",
    "    print('Rating set for unpredicted ratings = {}'. format(UNPRED_RATING))\n",
    "elif UNPRED_RATING == -1:\n",
    "    UNPRED_RATING = 3\n",
    "    print('Rating set for unpredicted ratings = {} (default)'. format(UNPRED_RATING))\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for UNPRED_RATING')\n",
    "    \n",
    "if TRAIN_TEST_SPLIT > 0 and TRAIN_TEST_SPLIT < 1:\n",
    "    print('TRAIN_TEST_SPLIT = {}'.format(TRAIN_TEST_SPLIT))\n",
    "elif TRAIN_TEST_SPLIT == -1:\n",
    "    TRAIN_TEST_SPLIT = 0.2\n",
    "    print('TRAIN_TEST_SPLIT = 0.2 (default)')\n",
    "else:\n",
    "    print('ERROR: Incorrect values set for TRAIN_TEST_SPLIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset: done\n",
      "USERS = 943\n",
      "ITEMS = 1682\n",
      "All users and items have at least one rating! Good!\n",
      "SPARSITY (p) = 0.0290249433107\n",
      "Sym matrix : p is polynomially larger than 1/n, all guarantees applicable\n",
      "Check and set dataset : done\n"
     ]
    }
   ],
   "source": [
    "data_csr = read_data_csr(fname=DATA_PATH, delimiter=DELIMITER)\n",
    "\n",
    "if SIZE_REDUCTION:\n",
    "    data_csr = reduce_size_of_data_csr(data_csr)\n",
    "\n",
    "if data_csr.shape[0] == N_RATINGS:  # gives total no of ratings read; useful for verification\n",
    "    print('Reading dataset: done')\n",
    "else:\n",
    "    print('Reading dataset: FAILED')\n",
    "    print( '# of missing ratings: ' + str(N_RATINGS - data_csr.shape[0]))\n",
    "    \n",
    "check_and_set_data_csr(data_csr=data_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train test split: done\n"
     ]
    }
   ],
   "source": [
    "[train_data_csr, test_data_csr] = generate_train_test_split_csr(data_csr=data_csr, split=TRAIN_TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize ratings: done\n",
      "CSR to symmetric CSR matrix: done\n"
     ]
    }
   ],
   "source": [
    "train_data_csr = normalize_ratings_csr(train_data_csr)\n",
    "train_data_csr = csr_to_symmetric_csr(train_data_csr)\n",
    "# the symmetric matrix obtained doesnt contain repititions for any user item pair\n",
    "# only the item_ids are scaled by item_ids += USERS\n",
    "# hence, we can safely go ahead and use this CSR matrix for sample splitting step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using THE algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Sample splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample splitting: done\n"
     ]
    }
   ],
   "source": [
    "[m1_csr, m2_csr, m3_csr] = sample_splitting_csr(data_csr=train_data_csr, c1=C1, c2=C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Expanding the Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph as dictionary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [00:02<00:00, 1050.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbor boundary matrix at 7-hop distance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [30:10<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbor boundary matrix at 8-hop distance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [32:02<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "[r_neighbor_matrix, r1_neighbor_matrix] = generate_neighbor_boundary_matrix(m1_csr)\n",
    "# all neighbor boundary vector for each user u is stored as u'th row in neighbor_matrix\n",
    "# though here the vector is stored a row vector, we will treat it as column vector in Step 4\n",
    "# Note: we might expect neighbor matrix to be symmetric with dimensions (USERS+ITEMS)*(USERS+ITEMS)\n",
    "#     : since distance user-item and item-user should be same\n",
    "#     : but this is not the case since there might be multiple paths between user-item\n",
    "#     : and the random path picked for user-item and item-user may not be same\n",
    "#     : normalizing the matrix also will result to rise of difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively choose RADIUS value for next run of algorithm:\n",
      "Showing distribution of count of neighbors for every vertex:\n",
      "                 0\n",
      "count  2625.000000\n",
      "mean      0.001524\n",
      "std       0.047794\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       0.000000\n",
      "max       2.000000\n"
     ]
    }
   ],
   "source": [
    "describe_neighbor_count(r_neighbor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively choose RADIUS value for next run of algorithm:\n",
      "Showing distribution of count of neighbors for every vertex:\n",
      "            0\n",
      "count  2625.0\n",
      "mean      0.0\n",
      "std       0.0\n",
      "min       0.0\n",
      "25%       0.0\n",
      "50%       0.0\n",
      "75%       0.0\n",
      "max       0.0\n"
     ]
    }
   ],
   "source": [
    "describe_neighbor_count(r1_neighbor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Computing the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating distance matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [25:49:16<00:00, 35.41s/it]   \n"
     ]
    }
   ],
   "source": [
    "distance_matrix = compute_distance_matrix(r_neighbor_matrix, r1_neighbor_matrix, m2_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2625, 2625)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(distance_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively choose THRESHOLD value in next step:\n",
      "Showing distribution of non zero (or observed) entries of distance matrix:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-55eea2490077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribe_distance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-046ad2c5a3a8>\u001b[0m in \u001b[0;36mdescribe_distance_matrix\u001b[0;34m(distance_matrix)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'To effectively choose THRESHOLD value in next step:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Showing distribution of non zero (or observed) entries of distance matrix:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sasahoo/anaconda2/envs/play/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[1;32m   5879\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5880\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5881\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot describe a DataFrame without columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5883\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpercentiles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": [
    "describe_distance_matrix(distance_matrix)\n",
    "# The error below is expected as we did not observe any distance values between any two vertices\n",
    "# as no vertex has neighbors at r=7 or r=8 hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Averaging datapoints to produce final estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prefer to choose a threshold now based on describe_distance_matrix\n",
    "THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_matrix = generate_sim_matrix(distance_matrix, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the test dataset using Model preparation section functions\n",
    "test_data_csr = normalize_ratings_csr(test_data_csr)\n",
    "test_data_csr = csr_to_symmetric_csr(test_data_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting estimates for only test data points\n",
    "prediction_array = generate_averaged_prediction_array(sim_matrix, m3_csr, test_data_csr)\n",
    "\n",
    "# To generate complete rating matrix do the following:\n",
    "#prediction_matrix = generate_averaged_prediction_matrix(sim_matrix, m3_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have already prepared the test data (required for our algorithm)\n",
    "y_actual  = test_data_csr[:,2]\n",
    "y_predict = prediction_array\n",
    "# If we want, we could scale our ratings back to 1 - 5 range for evaluation purposes\n",
    "#But then paper makes no guarantees about scaled ratings\n",
    "#y_actual  = y_actual * 5\n",
    "#y_predict = y_predict * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_rmse(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_avg_err(y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_mse(m1_csr, y_actual, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime.now().time()     # (hour, min, sec, microsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
